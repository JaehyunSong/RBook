# データハンドリング [拡張] {#sec-datahandling2}

```{r}
#| include: false
source("_common.R")
pacman::p_load(tidyverse, gt, gtExtras)
```

前章ではデータの一部 (subset)を抽出する方法について説明しましたが、本章はデータを拡張する、あるいは全く別のデータが得られるような処理について解説します。後者は主に元のデータを要約し (記述統計量)、その結果を出力する方法で、前者はデータ内の変数に基づき、指定された計算を行った結果を新しい列として追加する方法です。今回も[前章と同じデータ](Data/Ramen.csv)を使用します。

```{r}
#| message: false
pacman::p_load(tidyverse)
# データのパスは適宜修正すること
# 文字化けが生じる場合、以下のコードに書き換える。
# df <- read_csv("Data/Ramen.csv", locale = locale(encoding = "utf8"))
df <- read_csv("Data/Ramen.csv")
```

データの詳細については第[-@sec-handling1-select]章を参照してください。

## 記述統計量の計算 {#sec-handling2-summarise}

### `summarise()`による記述統計量の計算

ある変数の平均値や標準偏差、最小値、最大値などの記述統計量 (要約統計量)を計算することも可能です。これは`summarize()`または`summarise()`関数を使いますが、この関数は後で紹介する`group_by()`関数と組み合わせることで力を発揮します。ここではグルーピングを考えずに、全データの記述統計量を計算する方法を紹介します。

`summarise()`関数の使い方は以下の通りです。

```{r}
#| eval: false
# summarise()関数の使い方
データフレーム名 |>
  summarise(新しい変数名 = 関数名(計算の対象となる変数名))
```

もし、`Score`変数の平均値を計算し、その結果を`Mean`という列にしたい場合は以下のようなコードになります。

```{r}
df |>
  summarise(Mean = mean(Score))
```

ただし、`mean()`関数は欠損値が含まれるベクトルの場合、`NA`を返します。この場合方法は2つ考えられます。

1. `filter()`関数を使って`Score`が欠損しているケースを予め除去する。
2. `na.rm`引数を指定し、欠損値を除去した平均値を求める。

ここでは2番目の方法を使います。

```{r}
df |>
  summarise(Mean = mean(Score, na.rm = TRUE))
```

`df`の`Score`変数の平均値は`r round(mean(df$Score), 2)`であることが分かります。また、`summarise()`関数は複数の記述統計量を同時に計算することも可能です。以下は`Score`変数の平均値、中央値、標準偏差、最小値、最大値、第一四分位点、第三四分位点を計算し、`Score.Desc`という名のデータフレームに格納するコードです。

```{r}
Score.Desc <- df |>
  summarize(Mean   =     mean(Score,       na.rm = TRUE),  # 平均値
            Median =   median(Score,       na.rm = TRUE),  # 中央値
            SD     =       sd(Score,       na.rm = TRUE),  # 標準偏差
            Min    =      min(Score,       na.rm = TRUE),  # 最小値
            Max    =      max(Score,       na.rm = TRUE),  # 最大値
            Q1     = quantile(Score, 0.25, na.rm = TRUE),  # 第一四分位点
            Q3     = quantile(Score, 0.75, na.rm = TRUE))  # 第三四分位点
```

```{r}
Score.Desc
```

むろん、複数の変数に対して記述統計量を計算することも可能です。たとえば、平均予算 (`Budget`)、口コミ数 (`ScoreN`)、口コミ評価 (`Score`)の平均値を求めるとしたら、

```{r}
df |>
  summarize(Budget_Mean = mean(Budget, na.rm = TRUE), # 平均予算の平均値
            SocreN_Mean = mean(ScoreN, na.rm = TRUE), # 口コミ数の平均値
            Score_Mean  = mean(Score,  na.rm = TRUE)) # 評価の平均値
```

のように書きます。実は`summarise()`はこれくらいで十分便利です。ただし、以上の操作はもっと簡単なコードに置換できます。ただし、ラムダ式など、やや高度な内容になるため、以下の内容は飛ばして、次の節 (グルーピング)を読んでいただいても構いません。

まずは、複数の変数に対して同じ記述統計量を求める例を考えてみましょう。たとえば、`Budget`、`ScoreN`、`Score`に対して平均値を求める例です。これは`across()`関数を使うとよりコードが短くなります。まずは`across()`関数の書き方から見ましょう。

```{r}
#| eval: false
# across()の使い方
データフレーム名 |>
  summarise(across(変数名のベクトル, ラムダ式))
```

**変数名のベクトル**は長さ1以上のベクトルです。たとえば、`Budget`、`ScoreN`、`Score`の場合`c(Budget, ScoreN, Score)`になります。これは`df`内で隣接する変数ですから`Budget:Score`の書き方も使えます。また、`where()`や`any_of()`、`starts_with()`のような関数を使って変数を指定することも可能です。**ラムダ式**は`~`で始まる関数であり、匿名関数とよ呼ばれます。たとえば、平均値を計算するラムダ式は`~mean()`であり、第一引数は`.x`となります。この`.x`に`across()`の第一引数（変数たち）が入ります。また、`()`内には更に引数を指定することもでき、たとえば欠損値を除外して計算する`na.rm = TRUE`などが追加できます。ラムダ式の詳細は後でもう一度解説するとし、とりあえず`Budget`、`ScoreN`、`Score`の平均値を計算してみましょう[^lambda-another]。

[^lambda-another]: もう一つのラムダ式の書き方があり、今回の例だと、`\(x) mean(x, na.rm = TRUE)`とように書くこともできます。今回は`.x`でなく、`x`であることに注意しましょう。

```{r}
df |>
  summarize(across(Budget:Score, ~mean(.x, na.rm = TRUE)))
```

`across()`使わない場合、4行必要だったコードが2行になりました。変数が少ない場合は`across()`を使わない方が、可読性が高くなる場合もあります。しかし、変数が多くなる場合、可読性がやや落ちても`across()`を使った方が効率的でしょう。

次は、ある変数に対して複数の記述統計量を計算したい場合について考えます。`Budget`、`ScoreN`、`Score`変数の第一四分位点と第三四分位点を`across()`を使わずに計算すると家のような7行のコードになります。

```{r}
df |>
  summarize(Budget_Q1 = quantile(Budget, 0.25, na.rm = TRUE),
            Budget_Q3 = quantile(Budget, 0.75, na.rm = TRUE),
            ScoreN_Q1 = quantile(ScoreN, 0.25, na.rm = TRUE),
            ScoreN_Q3 = quantile(ScoreN, 0.75, na.rm = TRUE),
            Score_Q1  = quantile(Score,  0.25, na.rm = TRUE),
            Score_Q3  = quantile(Score,  0.75, na.rm = TRUE))
```

この作業も`across()`を使ってより短縮することができます。ここではラムダ式の知識が必要になります。ラムダ関数とは関数名を持たない[無名関数 (anonymous functions)](https://ja.wikipedia.org/wiki/無名関数)を意味しますが、詳細は割愛します。興味のある読者は[Wikipedia](https://ja.wikipedia.org/wiki/無名関数)などを参照してください。簡単にいうとその場で即席に関数を作成し、計算が終わったら破棄する関数です。ただ、Rは基本的にラムダ式を提供しているのではなく、`purrr`パッケージのラムダ式スタイルを使用します。まずは、書き方から確認します。

```{r}
#| eval: false
# ラムダ式を用いたacross()の使い方
データフレーム名 |>
  summarise(across(変数名のベクトル, .fns = list(結果の変数名 = ラムダ式)))
```

先ほどの書き方と似ていますが、関数を複数書く必要があるため、今回は関数名をlist型にまとめ、`.fns`引数に指定します。そして、*結果の変数名*は結果として出力されるデータフレームの列名を指定する引数です。たとえば、`Mean`にすると結果は`元の変数名1_Mean`、`元の変数名2_Mean`...のように出力されます。そして、ラムダ式が実際の関数が入る箇所です。とりあえず今回はコードを走らせ、結果から確認してみましょう。

```{r}
df |>
  summarize(across(Budget:Score, 
                   .fns = list(Q1 = ~quantile(.x, 0.25, na.rm = TRUE),
                               Q3 = ~quantile(.x, 0.75, na.rm = TRUE))))
```

結果の列名が`Budget_Q1`、`Budget_Q3`、`ScoreN_Q1`...のようになり、それぞれの変数の第一四分位点と第三四分位点が出力されます。問題はラムダ式の方ですが、普通の関数に非常に近いことが分かります。`across()`内のラムダ式は`~関数名(.x, その他の引数)`のような書き方になります。関数名の前に`~`が付いていることに注意してください。分位数を求める関数は`quantile()`であり、`quantile(ベクトル, 分位数)`であり、必要に応じて`na.rm`を付けます。この分位数が0.25なら第一四分位点、0.5なら第二四分位点 (=中央値)、0.75なら第三四分位点になります。それではラムダ式`~quantile(.x, 0.25, na.rm = TRUE)`はどういう意味でしょうか。これは`.x`の箇所に`Budget`や`ScoreN`、`Score`が入ることを意味します。`.x`という書き方は決まりです。`.y`とか`.Song-san-Daisuki`などはダメです。そして、`0.25`を付けることによって第一四分位点を出力するように指定します。また、`Budget`、`ScoreN`、`Score`に欠損値がある場合、無視するように`na.rm = TRUE`を付けます。

ラムダ式を第[-@sec-programming]章で解説した自作関数で表現すると、以下のようになります。

```{r}
#| eval: false
# 以下の3つは同じ機能をする関数である

# ラムダ式
~quantile(.x, 0.25, na.rm = TRUE)

# 一般的な関数の書き方1
名無し関数 <- function(x) {
  quantile(x, 0.25, na.rm = TRUE)
}

# 一般的な関数の書き方2
名無し関数 <- function(x) quantile(x, 0.25, na.rm = TRUE)
```

この3つは全て同じですが、ラムダ式は関数名を持たず、その場で使い捨てる関数です。むろん、ラムダ式を使わずに事前に第一四分位点と第三四分位点を求める関数を予め作成し、ラムダ式の代わりに使うことも可能です。まずは第一四分位点と第三四分位点を求める自作関数`FuncQ1`と`FuncQ2`を作成します。

```{r}
# ラムダ式を使わない場合は事前に関数を定義しておく必要がある
FuncQ1 <- function(x) {
  quantile(x, 0.25, na.rm = TRUE)
}
FuncQ3 <- function(x) {
  quantile(x, 0.75, na.rm = TRUE)
}
```

後は先ほどのほぼ同じ書き方ですが、今回はラムダ式を使わないため関数名に`~`を付けず、関数名のみで十分です。`()`も不要です。

```{r}
# やっておくと、summarise()文は簡潔になる
df |>
  summarize(across(Budget:Score, list(Q1 = FuncQ1, Q3 = FuncQ3)))
```

事前に関数を用意するのが面倒ですが、`across()`の中身はかなりスッキリしますね。もし、このような作業を何回も行うなら、ラムダ式を使わず、自作関数を用いることも可能です。ただし、自作関数であっても引数が2つ以上必要な場合はラムダ式を使います。

### `summarise()`に使える便利な関数

以下の内容は後で説明する`group_by()`関数を使っているため、まだ`group_by()`に馴染みのない読者はまずはここを読み飛ばし、グルーピングの節にお進みください。

**`IQR()`: 四分位範囲を求める**

四分位範囲は第三四分位点から第一四分位点を引いた値であり、Rの内蔵関数である`IQR()`を使えば便利です。この関数は`mean`や`sd()`関数と同じ使い方となります。

```{r}
df |>
  filter(!is.na(Walk)) |> # 予め欠損したケースを除くと、後でna.rm = TRUEが不要
  group_by(Pref) |>
  summarise(Mean    = mean(Walk),
            SD      = sd(Walk),
            IQR     = IQR(Walk),
            N       = n(),
            .groups = "drop") |>
  arrange(Mean)
```

**`first()`、`last()`、`nth()`: n番目の要素を求める**

稀なケースかも知れませんが、データ内、またはグループ内の`n`番目の行を抽出する時があります。たとえば、市区町村の情報が格納されているデータセットで、人口が大きい順でデータがソートされているとします。各都道府県ごとに最も人口が大きい市区町村のデータ、あるいは最も少ない市区町村のデータが必要な際、`first()`と`last()`関数が有効です。

それでは各都道府県ごとに「最も駅から遠いラーメン屋」の店舗名と最寄りの駅からの徒歩距離を出力したいとします。まずは、徒歩距離のデータが欠損しているケースを除去し、データを徒歩距離順でソートします。これは`filter()`と`arrange()`関数を使えば簡単です。続いて、`group_by()`を使って都府県単位でデータをグループ化します。最後に`summarise()`関数内に`last()`関数を使います。データは駅から近い順に鳴っているため、各都府県内の最後の行は駅から最も遠い店舗になるからです。

```{r}
df |>
  filter(!is.na(Walk)) |>
  arrange(Walk) |>
  group_by(Pref) |>
  summarise(Farthest  = last(Name),
            Distance  = last(Walk))
```

この`last()`を`first()`に変えると、最寄りの駅から最も近い店舗情報が表示されます。また、「`n`番目の情報」が必要な際は`nth()`関数を使います。`nth(Name, 2)`に変えることで2番目の店舗名が抽出できます。

**`n_distinct()`: ユニーク値の個数を求める**

`n_distinct()`は何種類の要素が含まれているかを計算する関数であり、`length(unique())`関数と同じ機能をします。たとえば、以下の`myVec1`に対して何種類の要素があるかを確認してみましょう。

```{r}
myVec1 <- c("A", "B", "B", "D", "A", "B", "D", "C", "A")

unique(myVec1)
```

`myVec1`は`"A"`、`"B"`、`"D"`、`"C"`の要素で構成されていることが分かります。これが`myVec1`の**ユニーク値 (unique values)**です。そして、このユニーク値の個数を調べるために`length()`を使います。

```{r}
length(unique(myVec1))
```

これで`myVec1`は4種類の値が存在することが分かります。これと全く同じ機能をする関数が`n_distinct()`です。

```{r}
n_distinct(myVec1)
```

この関数を`summarise()`に使うことで、都府県ごとに駅の個数が分かります。あるいは「東京都内の選挙区に、これまでの衆院選において何人の候補者が存在したか」も分かります。ここでは`df`内の都府県ごとに駅の個数を計算してみましょう。最後の駅数が多い順でソートします。

```{r}
df |>
  filter(!is.na(Station)) |> # 最寄りの駅が欠損しているケースを除去
  group_by(Pref) |>
  summarise(N_Station = n_distinct(Station),
            .groups   = "drop") |>
  arrange(desc(N_Station))
```

当たり前かも知れませんが、駅数が最も多いのは東京都で次が大阪府であることが分かります。

**`any()`、`all()`: 条件に合致するか否かを求める**

`any()`と`all()`はベクトル内の全要素に対して条件に合致するか否かを判定する関数です。ただし、`any()`は一つの要素でも条件に合致すれば`TRUE`を、全要素が合致しない場合`FALSE`を返します。一方、`all()`は全要素に対して条件を満たせば`TRUE`、一つでも満たさない要素があれば`FALSE`を返します。以下は`any()`と`all()`の例です。

```{r}
myVec1 <- c(1, 2, 3, 4, 5)
myVec2 <- c(1, 3, 5, 7, 11)

any(myVec1 %% 2 == 0) # myVec1を2で割った場合、一つでも余りが0か
all(myVec1 %% 2 == 0) # myVec1を2で割った場合、全ての余りが0か
all(myVec2 %% 2 != 0) # myVec2を2で割った場合、全ての余りが0ではないか
```

それでは実際に`df`に対して`any()`と`all()`関数を使ってみましょう。一つ目は「ある都府県に最寄りの駅から徒歩60分以上の店舗が**一つでも**あるか」であり、二つ目は「ある都府県の店舗は**全て**最寄りの駅から徒歩30分以下か」です。それぞれの結果を`Over60`と`Within30`という列で出力してみましょう。

```{r}
df |>
  group_by(Pref) |>
  summarise(Over60   = any(Walk >= 60, na.rm = TRUE),
            Within30 = all(Walk <= 30, na.rm = TRUE),
            .groups  = "drop")
```

埼玉県と神奈川県において、最寄りの駅から徒歩60以上の店がありました。また、京都府、東京都、奈良県、和歌山県の場合、全店舗が最寄りの駅から徒歩30分以下ということが分かります。当たり前ですが`Over60`が`TRUE`なら`Within30`は必ず`FALSE`になりますね。

## グルーピング {#sec-handling2-group}

### `group_by()`によるグループ化

先ほどの`summarise()`関数は確かに便利ですが、特段に便利とも言いにくいです。`df`の`Score`の平均値を計算するだけなら、`summarise()`関数を使わない方が楽です。

```{r}
# これまでのやり方
df |>
  summarise(Mean = mean(Score, na.rm = TRUE))

# 普通にこれでええんちゃう?
mean(df$Score, na.rm = TRUE)
```

しかし、これをグループごとに計算するならどうでしょう。たとえば、`Score`の平均値を都府県ごとに計算するとします。この場合、以下のようなコードになります。

```{r}
mean(df$Score[df$Pref == "東京都"],   na.rm = TRUE)
mean(df$Score[df$Pref == "神奈川県"], na.rm = TRUE)
mean(df$Score[df$Pref == "千葉県"],   na.rm = TRUE)
mean(df$Score[df$Pref == "埼玉県"],   na.rm = TRUE)
mean(df$Score[df$Pref == "大阪府"],   na.rm = TRUE)
mean(df$Score[df$Pref == "京都府"],   na.rm = TRUE)
mean(df$Score[df$Pref == "兵庫県"],   na.rm = TRUE)
mean(df$Score[df$Pref == "奈良県"],   na.rm = TRUE)
mean(df$Score[df$Pref == "和歌山県"], na.rm = TRUE)
```

変わったのは`df$Score`が`df$Score[df$Pref == "東京都"]`に変わっただけです。`df$Pref`が`"東京都"`であるか否かを`TRUE`と`FALSE`で判定し、これを基準に`df$Score`を抽出する仕組みです。`df$Score`と`df$Pref`は同じデータフレームですから、このような書き方で問題ありません。

これだけでもかなり書くのが面倒ですが、これが47都道府県なら、あるいは200ヶ国ならかなり骨の折れる作業でしょう。ここで大活躍するのが`dplyr`パッケージの`group_by()`関数です。引数はグループ化する変数名だけです。先ほどの作業を`dplyr`を使うなら`Pref`変数でグループ化し、`summarise()`関数で平均値を求めるだけです。今回は`Score`だけでなく、`ScoreN`の平均値も求めてみましょう。そして、評価が高い順にソートもしてみます。

```{r}
# ScoreNとScoreの平均値をPrefごとに求める
df |>
  group_by(Pref) |>
  summarise(ScoreN_Mean = mean(ScoreN, na.rm = TRUE),
            Score_Mean  = mean(Score,  na.rm = TRUE)) |>
  arrange(desc(Score_Mean))
```

評判が最も高い都府県は和歌山県、最も低いのは神奈川県ですね。Songも和歌山ラーメンは井出系も車庫前系も好きです。しかし、大事なのは「井出系」と「車庫前系」といった分類が正しいかどうかではありません。コードが非常に簡潔となり、ソートなども自由自在であることです。都府県ごとに`ScoreN`と`Score`の平均値を求める場合、`dplyr()`を使わなかったら18行のコードとなり、ソートも自分でやる必要があります。一方、`group_by()`関数を使うことによってコードが5行になりました。

また、これは2020年6月に公開された`dplyr`1.0.0からの問題ですが、`group_by()`の後に`summarise()`を使うと以下のようなメッセージが出力されます。

```
## `summarise()` ungrouping output (override with `.groups` argument)
```

これは`group_by()`で指定された変数のグループ化が自動的に解除されたことを意味します。なぜなら`summarise()`をする際は`Pref`をグループ変数として使いましたが、出力された結果の`Pref`変数はもはやグループとして機能できなくなるからです。元の`df`には`Pref`が`"東京都"`だったケースが1000行、`"京都府"`だったのが414行あったので、`Pref`変数でグループ化する意味がありました。しかし、`summarise()`から得られたデータフレームは`Pref == "東京都"`の行が1つしかありません。これはグループ化する意味がなくなったことを意味します。したがって、自動的にグループを解除してくれます。自動的にやってくれるのはありがたいことですが、可能ならば関数内に自分で明記することが推奨されます。そこで使う引数が`.groups`であり、`"drop"`を指定すると**全ての**グループ化変数を解除します。以下のようなコードだと先ほどのメッセージが表示されません。今後、意識的に入れるようにしましょう。

```{r}
# ScoreNとScoreの平均値をPrefごとに求める
df |>
  group_by(Pref) |>
  summarise(ScoreN_Mean = mean(ScoreN, na.rm = TRUE),
            Score_Mean  = mean(Score,  na.rm = TRUE),
            .groups     = "drop") |>
  arrange(desc(Score_Mean))
```

続いて、一つ便利な関数を紹介します。それはグループのサイズを計算する関数、`n()`です。この関数を`summarise()`内に使うと、各グループに属するケース数を出力します[^n]。先ほどのコードを修正し、各グループのサイズを`N`という名の列として追加してみましょう。そしてソートの順番は`N`を最優先とし、同じ場合は`Score_Mean`が高い方を上に出力させます。また、`ScoreN_Mean`の前に、口コミ数の合計も出してみましょう。

[^n]: 今回の例のように記述統計量とケース数を同時に計算する場合は`n()`を使いますが、ケース数**のみ**を計算する場合は`group_by()`と`summarise()`と組み合わせず、`count()`関数だけ使うことも可能です。グループ化変数（ここでは`Pref`）を`count()`の引数として指定すると、`Pref`の値ごとのケース数が表示されます。

```{r}
# Prefごとに口コミ数の合計、口コミ数の平均値、評価の平均値、店舗数を求める
# 店舗数-評価の平均値順でソートする
df |>
  group_by(Pref) |>
  summarise(ScoreN_Sum  = sum(ScoreN,  na.rm = TRUE),
            ScoreN_Mean = mean(ScoreN, na.rm = TRUE),
            Score_Mean  = mean(Score,  na.rm = TRUE),
            N           = n(),
            .groups     = "drop") |>
  arrange(desc(N), desc(Score_Mean))
```

記述統計をグループごとに求めるのは普通にあり得るケースですし、実験データの場合はほぼ必須の作業でう。統制群と処置群間においてグループサイズが均一か、共変量のバラツキが十分に小さいかなどを判断する際に`group_by()`と`summarise()`関数の組み合わせは非常に便利です。

### 複数の変数を用いたグループ化

グループ化変数は2つ以上指定することも可能です。たとえば、都府県 (`Pref`)と最寄りの駅の路線 (`Line`)でグループ化することも可能です。それでは`Pref`と`Line`でグループ化し、店舗数と口コミ数、評価の平均値を計算し、ソートの順番は店舗数、店舗数が同じなら評価の平均値が高い順にしましょう。今回も`summarise()`内に`.group = "drop"`を指定し、グループ化を解除します。今回はTop 20まで出してみましょう。

```{r}
# ScoreNとScoreの平均値をPrefごとに求める
df |>
  filter(!is.na(Line)) |> # Lineが欠損していないケースのみ残す
  group_by(Pref, Line) |> # PrefとLineでグループ化
  summarise(N           = n(),
            ScoreN_Sum  = sum(ScoreN,  na.rm = TRUE),
            Score_Mean  = mean(Score,  na.rm = TRUE),
            .groups     = "drop") |>
  arrange(desc(N), desc(Score_Mean)) |>
  print(n = 20)
```

ぐるなびに登録されているラーメン屋が最も多い路線は埼玉県内の東武東上線で122店舗があります。東武東上線は東京都と埼玉県をまたがる路線ですので、東武東上線だけならもっと多いかも知れませんね。

ここで一つ考えたいのは`summarise()`内の`.groups`引数です。前回はグループ化に使った変数ごとに1行しか残っていなかったのでグループ化を全て解除しました。しかし、今回は状況がやや異なります。グループ化変数に使った`Pref`を考えると、まだ`Pref == "東京都"`であるケースがいくつかあります。やろうとすればまだグループ化出来る状態です。これは`Line`についても同じです。`Line == "東武東上線"`の行はここには表示されていないものの、まだデータに残っています。もし、これ以上グループ化しないなら今のように`.groups = "drop"`が正しいですが、もしもう一回グループ化したい場合はどうすればよいでしょうか。方法は2つ考えられます。

1. もう一度パイプ演算子を使って`group_by()`関数を使う (以下の9行目)。
    * 結果を見ると`## # Groups:   Pref, Line [523]`で、ちゃんとグループ化されていることが分かります。

```{r}
df |>
  filter(!is.na(Line)) |> 
  group_by(Pref, Line) |> 
  summarise(N           = n(),
            ScoreN_Sum  = sum(ScoreN,  na.rm = TRUE),
            Score_Mean  = mean(Score,  na.rm = TRUE),
            .groups     = "drop") |>
  arrange(desc(N), desc(Score_Mean)) |>
  group_by(Pref, Line) |> # group_by()、もう一度
  print(n = 5)
```

2. `.groups`引数を何とかする。

推奨される方法は2番です。具体的には`.groups = "keep"`を指定するだけであり、こっちの方が無駄なコードを省けることができます。

```{r}
df |>
  filter(!is.na(Line)) |> 
  group_by(Pref, Line) |> 
  summarise(N           = n(),
            ScoreN_Sum  = sum(ScoreN,  na.rm = TRUE),
            Score_Mean  = mean(Score,  na.rm = TRUE),
            .groups     = "keep") |>
  arrange(desc(N), desc(Score_Mean)) |>
  print(n = 5)
```

`.groups`引数は`"drop"`と`"keep"`以外にも`"drop_last"`があります。実は`summarise()`に`.groups`引数を指定したい場合のデフォルト値は`.groups == "drop_last"`または`"keep"`ですが、ここがややこしいです。主なケースにおいてデフォルト値は`"drop"`となりますとなります。`.groups == "drop_last"`これは最後のグループ化変数のみ解除する意味です。今回の例だと、2番目のグループ化変数である`Line`がグループ化変数から外され、`Pref`のみがグループ化変数として残る仕組みです。

それではデフォルト値が`"keep"`になるのはいつでしょうか。それは記述統計量の結果が長さ2以上のベクトルである場合です。平均値を求める`mean()`、標準偏差を求める`sd()`などは、結果として長さ1のベクトルを返します。しかし、長さ2以上ののベクトルを返す関数もあります。たとえば、分位数を求める`quantile()`関数があります。`quantile(ベクトル名, 0.25)`の場合、第一四分位点のみ返すため、結果は長さ1のベクトルです。しかし、`quantile(ベクトル名, c(0.25, 0.5, 0.75))`のように第一四分位点から第三四分位点を同時に計算し、長さ3のベクトルが返されるケースもありますし、第二引数を省略すると、最小値・第一四分位点・第二四分位点・第三四分位点・最大値、つまり、長さ5のベクトルが返される場合があります。

```{r}
# 第一四分位点のみを求める (長さ1のベクトル)
quantile(df$Walk, 0.25, na.rm = TRUE)

# 引数を省略する (長さ5のベクトル)
quantile(df$Walk, na.rm = TRUE)
```

`.groups`のデフォルト値が`"keep"`になるのは、このように長さ2以上のベクトルが返されるケースです。たとえば、都府県と最寄りの駅の路線でグループ化し、店舗までの徒歩距離の平均値を求めるとします。デフォルト値の変化を見るために、ここではあえて`.groups`引数を省略しました。

```{r}
df |>
  filter(!is.na(Walk)) |>
  group_by(Pref, Line) |>
  summarise(Mean = mean(Walk))
```

最初は`Pref`と`Line`でグループ化しましたが、`summarise()`の後、`Line`がグループ化変数から外されました。つまり、引数が`"drop_last"`になっていることです。

それでは、平均値に加えて、第一四分位点と第三四分位点も計算し、`Quantile`という名で格納してみましょう。

```{r}
df |>
  filter(!is.na(Walk)) |>
  group_by(Pref, Line) |>
  summarise(Mean     = mean(Walk),
            Quantile = quantile(Walk, c(0.25, 0.75)))
```

同じ`Pref`、`Line`のケースが2つずつ出来ています。最初に来る数値は第一四分位点、次に来るのが第三四分位点です。そして最初のグループ化変数であった`Pref`と`Line`が、`summarise()`後もグループ化変数として残っていることが分かります。

`.groups`引数は記述統計量だけを計算する意味ではあまり意識する必要がありません。しかし、得られた記述統計量から何らかの計算をしたり、さらにもう一回記述統計量を求めたりする際、予期せぬ結果が得られる可能性があるため注意する必要があります。出来る限り`.groups`引数は指定するようにしましょう。

## 変数の計算 {#sec-handling2-mutate}

### `mutate()`関数の使い方

続いて、データフレーム内の変数を用いて計算を行い、その結果を新しい列として格納する`mutate()`関数について紹介します。まず、`mutate()`関数の書き方からです。

```{r}
#| eval: false
# mutate()関数の使い方
データフレーム名 |>
  mutate(新しい変数名 = 処理内容)
```

これは何らかの処理を行い、その結果を新しい変数としてデータフレームに追加することを意味します。新しく出来た変数は、基本的に最後の列になります。ここでは分単位である`Walk`を時間単位に変換した`Walk_Hour`変数を作成するとします。処理内容は`Walk / 60`です。最後に、都府県名、店舗名、徒歩距離 (分)、徒歩距離 (時間)のみを残し、遠い順にソートします。

```{r}
df |>
  filter(!is.na(Walk)) |>
  mutate(Walk_Hour = Walk / 60) |>
  select(Pref, Name, Walk, Walk_Hour) |>
  arrange(desc(Walk_Hour))
```

`mutate()`は3行目に登場しますが、これは`Walk`を60に割った結果を`Walk_Hour`としてデータフレームの最後の列として格納することを意味します。もし、最後の列でなく、ある変数の前、または後にしたい場合は、`.before`または`.after`引数を追加します。これは`select()`関数の`.before`と`.after`と同じ使い方です。たとえば、新しく出来た`Walk_Hour`を`ID`と`Name`の間に入れたい場合は

```{r}
#| eval: false
# コードの3行名を修正 (.before使用)
mutate(Walk_Hour = Walk / 60,
       .before   = Name)

# コードの3行名を修正 (.after使用)
mutate(Walk_Hour = Walk / 60,
       .after    = ID)
```

のようにコードを修正します。

むろん、変数間同士の計算も可能です。たとえば、以下のような`df2`があり、1店舗当たりの平均口コミ数を計算し、`ScoreN_Mean`という変数名で`ScoreN_Sum`の後に格納うするとします。この場合、`ScoreN_Sum`変数を`N`で割るだけです。

```{r}
df2 <- df |>
  group_by(Pref) |>
  summarise(Budget_Mean = mean(Budget, na.rm = TRUE),
            ScoreN_Sum  = sum(ScoreN, na.rm = TRUE),
            Score_Mean  = mean(Score, na.rm = TRUE),
            N           = n(),
            .groups     = "drop")
```

```{r}
df2 |>
  mutate(ScoreN_Mean = ScoreN_Sum / N,
         .after      = ScoreN_Sum)
```

このように、データ内の変数を用いた計算結果を新しい列として追加する場合は、`mutate()`が便利です。これを`mutate()`を使わずに処理する場合、以下のようなコードになりますが、可読性が相対的に低いことが分かります。

```{r}
#| eval: false
df2$ScoreN_Mean <- df2$ScoreN_Sum / df2$N
df2 <- df2[, c("Pref", "Budget_Mean", "Walk_Mean", 
               "ScoreN_Sum", "ScoreN_Mean", "Score_Mean", "N")]
```

むろんですが、計算には`+`や`/`のような演算子だけでなく、関数を使うことも可能です。たとえば、`Budget`が1000円未満なら`"Cheap"`、1000円以上なら`"Expensive"`と示す変数`Budget2`を作成する場合は`ifelse()`関数が使えます。

```{r}
df |> 
  mutate(Budget2 = ifelse(Budget < 1000, "Cheap", "Expensive")) |>
  filter(!is.na(Budget2)) |> # Budget2が欠損した店舗を除外
  group_by(Pref, Budget2) |> # PrefとBudget2でグループ化
  summarise(N = n(),          # 店舗数を表示
            .groups = "drop")
```

これは各都府県ごとの予算1000円未満の店と以上の店の店舗数をまとめた表となります。もし、500円未満なら`"Cheap"`、500円以上~1000円未満なら`"Reasonable"`、1000円以上なら`"Expensive"`になる`Budget3`変数を作るにはどうすればよいでしょうか。第[-@sec-programming]章で紹介しました`ifelse()`を重ねることも出来ますが、ここでは`case_when()`関数が便利です。まずは、`ifelse()`を使ったコードは以下の通りです。

```{r}
#| eval: false
# ifelse()を使う場合
df |> 
  mutate(Budget3 = ifelse(Budget < 500, "Cheap", 
                          ifelse(Budget >= 500 & Budget < 1000, "Reasonable",
                                 "Expensive"))) |>
  filter(!is.na(Budget3)) |>
  group_by(Pref, Budget3) |>
  summarise(N = n(),         
            .groups = "drop")
```

`case_when()`を使うと以下のような書き方になります。

```{r}
#| eval: false
# case_when()を使う場合
df |> 
  mutate(Budget3 = case_when(Budget < 500                  ~ "Cheap",
                             Budget >= 500 & Budget < 1000 ~ "Reasonable",
                             Budget >= 1000                ~ "Expensive"),
         # 新しく出来た変数をfactor型にその場で変換することも可能
         Budget3 = factor(Budget3, 
                          levels = c("Cheap", "Reasonable", "Expensive"))) |>
  filter(!is.na(Budget3)) |>
  group_by(Pref, Budget3) |>
  summarise(N = n(),         
            .groups = "drop")
```

書く手間の観点では`case_when()`は`ifelse()`と大きく違いはないかも知れませんが、コードが非常に読みやすくなっています。`case_when()`関数の書き方は以下の通りです。

```{r}
#| eval: false
# case_when()の使い方
データフレーム名 |>
  mutate(新変数名 = case_when(条件1 ~ 条件1を満たす場合の結果値, 
                             条件2 ~ 条件2を満たす場合の結果値, 
                             条件3 ~ 条件3を満たす場合の結果値, 
                             ...))
```

似たような機能をする関数として`recode()`関数があります[^handling2-recode]。これは変数の値を単純に置換したい場合に便利な関数です。たとえば、都府県名をローマ字に変換するケースを考えてみましょう。

[^handling2-recode]: `recode()`関数は他にも{car}パッケージでも提供されています。{car}も広く使われている依存パッケージの一つであるため、`dplyr::recode()`と明示した方が良いかも知れません。

```{r}
# recode()を使う場合
df2 |> 
  mutate(Pref2 = recode(Pref,
                        "東京都"   = "Tokyo",
                        "神奈川県" = "Kanagawa",
                        "千葉県"   = "Chiba",
                        "埼玉県"   = "Saitama",
                        "大阪府"   = "Osaka",
                        "京都府"   = "Kyoto",
                        "兵庫県"   = "Hyogo",
                        "奈良県"   = "Nara",
                        "和歌山県" = "Wakayama",
                        .default  = "NA"))
```

使い方は非常に直感的です。

```{r}
#| eval: false
# recode()の使い方
データフレーム名 |>
  mutate(新変数名 = recode(元の変数名,
                            元の値1 =  新しい値1, 
                            元の値2 =  新しい値2, 
                            元の値3 =  新しい値3, 
                            ...,
                            .default = 該当しない場合の値))
```

最後の`.default`引数は、もし該当する値がない場合に返す値を意味し、長さ1のベクトルを指定します。もし、指定しない場合は`NA`が表示されます。また、ここには紹介しておりませんでしたが、`.missing`引数もあり、これは欠損値の場合に返す値を意味します。

もう一つ注意すべきところは、今回はcharacter型変数をcharacter型へ変換したため、「`"東京都" = "Tokyo"`」のような書き方をしました。しかし、numeric型からcharacter型に変換する場合は数字の部分を`` ` ``で囲む必要があります。たとえば、「`` `1` = "Tokyo"``」といった形式です。ただし、character型からnumeric型への場合は「`"東京都" = 1`」で構いません。

`recode()`は値をまとめる際にも便利です。たとえば、`EastJapan`という変数を作成し、関東なら`1`を、それ以外なら`0`を付けるとします。そして、これは`Pref`変数の後に位置づけます。

```{r}
# 都府県を関東か否かでまとめる
df2 |> 
  mutate(EastJapan = recode(Pref,
                            "東京都"   = 1,
                            "神奈川県" = 1,
                            "千葉県"   = 1,
                            "埼玉県"   = 1,
                            "大阪府"   = 0,
                            "京都府"   = 0,
                            "兵庫県"   = 0,
                            "奈良県"   = 0,
                            "和歌山県" = 0,
                            .default  = 0),
         .after = Pref)
```

ただし、関東以外は全て0になるため、以下のように省略することも可能です。

```{r}
# .default引数を指定する場合
df3 <- df2 |> 
  mutate(EastJapan = recode(Pref,
                            "東京都"   = 1,
                            "神奈川県" = 1,
                            "千葉県"   = 1,
                            "埼玉県"   = 1,
                            .default  = 0),
         .after = Pref)

df3
```

新しく出来た`EastJapan`のデータ型はなんでしょうか。

```{r}
class(df3$EastJapan)
```

`EastJapan`はnumeric型ですね。もし、これをfactor型にしたい場合はどうすればよいでしょうか。それは`mutate()`内で`EastJapan`を生成した後に`factor()`関数を使うだけです。

```{r}
# EastJapan変数をfactor型にする
df3 <- df2 |> 
  mutate(EastJapan = recode(Pref,
                            "東京都"   = 1,
                            "神奈川県" = 1,
                            "千葉県"   = 1,
                            "埼玉県"   = 1,
                            .default  = 0),
         EastJapan = factor(EastJapan, levels = c(0, 1)),
         .after = Pref)

df3$EastJapan
```

`EastJapan`がfactor型になりました。実は、`recode`は再コーディングと同時にfactor化をしてくれる機能があります。ただし、`recode()`関数でなく、`recode_factor()`関数を使います。

```{r}
# recode_factor()を使う方法
df3 <- df2 |> 
  mutate(EastJapan = recode_factor(Pref,
                                   "東京都"   = 1,
                                   "神奈川県" = 1,
                                   "千葉県"   = 1,
                                   "埼玉県"   = 1,
                                   .default  = 0),
         .after = Pref)

df3$EastJapan
```

ただし、levelの順番は`recode_factor()`内で定義された順番になることに注意してください。factor型のより詳細な扱いについては第[-@sec-factor]章で解説します。

## 行単位の操作 {#sec-handling2-rowwise}

ここでは行単位の操作について考えたいと思います。第[-@sec-handling1-select]章で使った`myDF1`を見てみましょう。

```{r}
myDF1 <- data.frame(
  ID  = 1:5,
  X1  = c(2, 4, 6, 2, 7),
  Y1  = c(3, 5, 1, 1, 0),
  X1D = c(4, 2, 1, 6, 9),
  X2  = c(5, 5, 6, 0, 2),
  Y2  = c(3, 3, 2, 3, 1),
  X2D = c(8, 9, 5, 0, 1),
  X3  = c(3, 0, 3, 0, 2),
  Y3  = c(1, 5, 9, 1, 3),
  X3D = c(9, 1, 3, 3, 8)
)

myDF1
```

ここで`X1`と`X2`と`X3`の平均値を計算し、`X_Mean`という名の変数にする場合、以下のような書き方が普通でしょう。

```{r}
myDF1 |>
  mutate(X_Mean = mean(c(X1, X2, X3)))
```

あら、なんかおかしくありませんか。1行目の場合、`X1`と`X2`、`X3`それぞれ2、5、3であり、平均値は3.333であるはずなのに3.133になりました。これは2行目以降も同じです。なぜでしょうか。

実は`dplyr`は行単位の計算が苦手です。実際、データフレームというのは既に説明したとおり、縦ベクトルを横に並べたものです。列をまたがる場合、データ型が異なる場合も多いため、そもそも使う場面も多くありません。したがって、以下のような書き方が必要でした。

```{r}
myDF1 |>
  mutate(X_Mean = (X1 + X2 + X3) / 3)
```

先ほどの`mean(c(X1, X2, X3))`は(`X1`列と`X2`列、`X3`列)の平均値です。`X1`は長さ1のベクトルではなく、その列全体を指すものです。つまり、`mean(c(X1, X2, X3))`は`mean(c(myD1F$X1, myDF1$X2, myDF1$X3))`と同じことになります。だから全て3.133という結果が得られました。ただし、後者はベクトル同士の加減乗除になるため問題ありません。実際`c(1, 2, 3) + c(3, 5, 0)`は同じ位置の要素同士の計算になることを既に第[-@sec-datastructure_vector]章で説明しました。

ここで`mean()`関数を使う場合には全ての演算を、一行一行に分けて行う必要があります。ある一行のみに限定する場合、`mean(c(X1, X2, X3))`の`X1`などは長さ1のベクトルになるため、`(X1 + X2 + X3) / 3`と同じことになります。この「一行単位で処理を行う」ことを指定する関数が`rowwise()`関数です。これは行単位の作業を行う前に指定するだけです。

```{r}
myDF1 |>
  rowwise() |>
  mutate(X_Mean = mean(c(X1, X2, X3)))
```

これで問題なく行単位の処理ができるようになりました。今回は変数が3つのみだったので、これで問題ありませんが、変数が多くなると`:`や`starts_with()`、`num_range()`などを使って変数を選択したくなります。この場合は計算する関数内に`c_across()`を入れます。ここでは`X1`列から`X3D`列までの平均値を求めてみましょう。

```{r}
myDF1 |>
  rowwise() |>
  mutate(X_Mean = mean(X1:X3D))
```

実は`rowwise()`関数、2020年6月に公開されたdplyr 1.0.0で注目された関数ですが、昔のdplyrにも`rowwise()`関数はありました。ただし、`purrr`パッケージや`tidyr`パッケージの`nest()`関数などにより使い道がなくなりましたが、なぜか華麗に復活しました。データ分析に使うデータは基本単位は列であるため、実際に`rowwise()`が使われる場面は今の段階では多くないでしょう。また、簡単な作業なら`X1 + X2`のような演算でも対応できます。それでも、覚えておけば便利な関数であることには間違いありません。

## データの結合 {#sec-handling2-merge}

### 行の結合

まずは、複数のデータフレームまたはtibbleを縦に結合する方法について解説します。イメージとしては @fig-handling2_merge_row のようなものです。

![行の結合](Figs/Handling2/Merge1.png){#fig-handling2_merge_row width="75%" fig-align="center"}

行を結合する際には`dplyr`パッケージの`bind_rows()`関数を使います。この関数の使い方は以下の通りです。

```{r}
#| eval: false
# 新しいデータ名ではなく、既にあるデータ名にすると上書きとなる
新しいデータ名 <-  bind_rows(データ1, データ2, ...)
```

それでは早速実際に使ってみましょう。実習のために、4つのtibbleを作成します (tibbleでなくデータフレームでも問題ありません)。

```{r}
# tibble()の代わりにdata.frame()も可
rbind_df1 <- tibble(X1 = 1:3,
                    X2 = c("A", "B", "C"),
                    X3 = c(T, T, F)) # TRUEとFALSEはTはFと省略可能

rbind_df2 <- tibble(X1 = 4:6,
                    X2 = c("D", "E", "F"),
                    X3 = c(F, T, F))

rbind_df3 <- tibble(X1 = 7:9,
                    X3 = c(T, T, T),
                    X2 = c("G", "H", "I"))

rbind_df4 <- tibble(X1 = 10:12,
                    X2 = c("J", "K", "L"),
                    X5 = c("Song", "Yanai", "Hadley"))

rbind_df1 # rbind_df1を出力
rbind_df2 # rbind_df2を出力
rbind_df3 # rbind_df3を出力
rbind_df4 # rbind_df4を出力
```

まずは、`rbind_df1`と`rbind_df2`を結合してみます。この2つのデータは同じ変数が同じ順番で並んでいますね。

```{r}
Binded_df1 <- bind_rows(rbind_df1, rbind_df2)
Binded_df1
```

2つのデータが結合されたことが確認できます。それでは`rbind_df1`と`rbind_df2`、`rbind_df3`はどうでしょうか。確かに3つのデータは同じ変数を持ちますが、`rbind_df3`は変数の順番が`X1`、`X3`、`X2`になっています。このまま結合するとエラーが出るでしょうか。とりあえず、やってみます。

```{r}
Binded_df2 <- bind_rows(rbind_df1, rbind_df2, rbind_df3)
Binded_df2
```

このように変数の順番が異なっても、先に指定したデータの変数順で問題なく結合できました。これまでの作業は{dplyr}パッケージの`bind_rows()`を使わずに、R内蔵関数の`rbind()`でも同じやり方でできます。`bind_rows()`の特徴は、変数名が一致しない場合、つまり今回の例だと`rbind_df4`が含まれる場合です。`rbind_df1`から`rbind_df3`までは順番が違っても`X1`、`X2`、`X3`変数で構成されていました。一方、`rbind_dr4`には`X3`がなく、新たに`X4`という変数があります。これを`rbind()`関数で結合するとエラーが出力されます。

```{r}
#| error: true
# rbind()を使う場合
rbind(rbind_df1, rbind_df2, rbind_df3, rbind_df4)
```

一方、`bind_rows()`はどうでしょうか。

```{r}
Binded_df3 <- bind_rows(rbind_df1, rbind_df2, rbind_df3, rbind_df4)
Binded_df3
```

`X1`から`X4`まで全ての列が生成され、元のデータにはなかった列に関しては`NA`で埋められています。

他にも`bind_rows()`には`.id`という便利な引数があります。これは結合の際に新しい列を追加し、結合前のデータごとの固有の値を割り当ててくれます。たとえば、`rbind_df1`と`rbind_df2`を結合し、それぞれ`"Data1"`、`"Data2"`という値を割り当てるとしましょう。この場合、2つの表を`list()`関数でまとめ、`.id`には新しく追加される列名を指定します。

```{r}
bind_rows(list("Data1" = rbind_df1, "Data2" = rbind_df2),
          .id = "Data")
```

予めリストを作っておいて、それを`bind_rows()`に渡すこともできます。

```{r}
rbind_list <- list("Data1" = rbind_df1, "Data2" = rbind_df2)

bind_rows(rbind_list, .id = "Data")
```

このように`bind_rows()`は`rbind()`よりも便利です。しかし、`bind_rows()`の完全勝利かというと、そうとは限りません。自分で架空した複数のデータフレーム、またはtibbleを結合する際、「このデータは全て同じ変数を持っているはず」と事前に分かっているなら`rbind()`の方が効果的です。なぜなら、変数名が異なる場合、エラーが出力されるからです。`bind_rows()`を使うと、コーディングミスなどにより、列名の相違がある場合でも結合してくれてしまうので、分析の結果を歪ませる可能性があります。

### 列の結合

実はデータ分析においてデータの結合といえば、列の結合が一般的です。これは @fig-handling2_merge_col のような操作を意味します。

![列の結合](Figs/Handling2/Merge2.png){#fig-handling2_merge_col width="75%" fig-align="center"}

まずは、本章で作成した`df2`をもう一回作ってみます。

```{r}
df2 <- df |>
  group_by(Pref) |>
  summarise(Budget_Mean = mean(Budget, na.rm = TRUE),
            ScoreN_Sum  = sum(ScoreN, na.rm = TRUE),
            Score_Mean  = mean(Score, na.rm = TRUE),
            N           = n(),
            .groups     = "drop")

df2
```

ラーメン屋の店舗ですが、たしかにデータには埼玉、東京、大阪などは1000店舗しか入っておりません。実はもっと多いですが、ぐるなびAPIの仕様上、最大1000店舗しか情報取得が出来ないからです。ここに実際の店舗数が入っている新しいデータセット、[Ramen2.csv](Data/Ramen2.csv)があります。これを読み込み、`df3`という名で格納しましょう。

```{r}
df3 <- read_csv("Data/Ramen2.csv")

df3
```

```{r}
#| label: tbl-handling2_dataset
#| echo: false
#| tbl-cap: "`Ramen2.csv`の詳細"
tibble(Var = c("`Pref`", "`Pop`", "`Area`", "`RamenN`", "`Turnout`", 
               "`LDP`", "`CDP`", "`DPFP`", "`Komei`", "`JIP`", 
               "`JCP`", "`SDP`", "`Reiwa`", "`NHK`", "`HRP`"),
       Desc = c("都道府県名",
                "日本人人口 (2015年国勢調査)",
                "面積 (2015年国勢調査)",
                "ぐるなびに登録されたラーメン屋の店舗数",
                "2019年参院選: 投票率 (比例)",
                "2019年参院選: 自民党の得票率 (比例)",
                "2019年参院選: 立憲民主党の得票率 (比例)",
                "2019年参院選: 国民民主党の得票率 (比例)",
                "2019年参院選: 公明党の得票率 (比例)",
                "2019年参院選: 日本維新の会の得票率 (比例)",
                "2019年参院選: 日本共産党の得票率 (比例)",
                "2019年参院選: 社会民主党の得票率 (比例)",
                "2019年参院選: れいわ新選組の得票率 (比例)",
                "2019年参院選: NHKから国民を守る党の得票率 (比例)",
                "2019年参院選: 幸福実現党の得票率 (比例)")) |>
  gt() |> 
  cols_label("Var" = "変数名", "Desc" = "説明") |> 
  fmt_markdown(columns = 1)
```

本データは都道府県ごとの人口、面積、ぐるなびに登録されたラーメン屋の店舗数、2019年参議院議員通常選挙の結果が格納されています。人口と面積は2015年国勢調査、ぐるなびの情報は2020年6月時点での情報です。

`df2`にデータ上の店舗数ではなく、実際の店舗数を新しい列として追加したい場合はどうすれば良いでしょうか。簡単な方法としては`df3`から情報を取得し、それを自分で入れる方法です。

```{r}
df3 |>
  filter(Pref %in% df2$Pref) |> # <1>
  select(Pref, RamenN)          # <2>
```

1. df2のPrefベクトルの要素と一致するものに絞る
2. 都道府県名とラーメン屋の店舗数のみ抽出

そして、この情報を`df2$RamenN <- c(415, 1106, 1254, ...)`のように追加すればいいですね。

しかし、このような方法は非効率的です。そもそも`df3`から得られた結果の順番と`df2`の順番も一致しないので、一々対照しながらベクトルを作ることになります。ここで登場する関数が`dplyr`の`*_join()`関数群です。この関数群には4つの関数が含まれており、以下のような使い方になります。

```{r}
#| eval: false
# 新しいデータ名ではなく、データ1またはデータ2の名前に格納すると上書きとなる

# 1. データ1を基準に結合
新しいデータ名 <-  left_join(データ1, データ2, by = "共通変数名")

# 2. データ2を基準に結合
新しいデータ名 <- right_join(データ1, データ2, by = "共通変数名")

# 3. データ1とデータ2両方に共通するケースのみ結合
新しいデータ名 <- inner_join(データ1, データ2, by = "共通変数名")

# 4. データ1とデータ2、どれかに存在するケースを結合
新しいデータ名 <-  full_join(データ1, データ2, by = "共通変数名")
```

4つの関数の違いについて説明する前に、`by`引数について話したいと思います。これは主にキー (key)変数と呼ばれる変数で、それぞれのデータに同じ名前の変数がある必要があります。`df2`と`df3`だとそれが`Pref`変数です。どの`*_join()`関数でも、`Pref`の値が同じもの同士を結合することになります。

データのキー変数名が異なる場合もあります。たとえば、データ1の都道府県名は`Pref`という列に、データ2の都道府県名は`Prefecture`という列になっている場合、`by = "Pref"`でなく、`by = c("データ1のキー変数名" = "データ2のキー変数名")`、つまり、`by = c("Pref" = "Prefecture")`と指定します。

それでは、`df3`から都道府県名とラーメン屋の店舗数だけ抽出し、`df4`として格納しておきます。

```{r}
df4 <- df3 |>
  select(Pref, RamenN)

df4
```

これから共通変数名の値をキー (key)と呼びます。今回の例だと`Pref`が`df2`と`df4`のキー変数であり、その値である`"東京都"`、`"北海道"`などがキーです。

まずは、`inner_join()`の仕組みについて考えます。これは`df2`と`df4`に共通するキーを持つケースのみ結合する関数です。`df4`には`"北海道"`というキーがありますが、`df2`にはありません。したがって、キーが`"北海道"`のケースは結合から除外されます。これをイメージにしたものが @fig-handling2_merge_inner です[^merge1]。それぞれ3 $\times$ 2 (3行2列)のデータですが、キーが一致するケースは2つしかないため、結合後のデータは3 $\times$ 2となります。

[^merge1]: これらの図は @Grolemund_Wickham:2016 を参考にしました。

![`inner_join()`の仕組み](Figs/Handling2/Merge_Inner.png){#fig-handling2_merge_inner width="75%" fig-align="center"}

実際にやってみましょう。

```{r}
inner_join(df2, df4, by = "Pref")
```

共通するキーは9つのみであり、結果として返されたデータの大きさも9 $\times$ 6です。`df2`に足された`df4`は2列のデータですが、キー変数である`Pref`は共通するため、1列のみ足されました。キー変数を両方残す場合は`keep = TRUE`引数を追加してください。

一方、`full_join()`は、すべてのキーに対して結合を行います ( @fig-handling2_merge_full )。たとえば、`df2`には`"北海道"`というキーがありません。それでも新しく出来上がるデータには北海道の列が追加されます。ただし、道内店舗の平均予算、口コミ数などの情報はないため、欠損値が代入されます。

![`full_join()`の仕組み](Figs/Handling2/Merge_Full.png){#fig-handling2_merge_full width="75%" fig-align="center"}

それでは実際、結果を確認してみましょう。今回は結合後、`RamenN`が大きい順で出力します。

```{r}
full_join(df2, df4, by = "Pref") |>
  arrange(desc(RamenN)) # ぐるなびに登録された店舗の多い都道府県から出力
```

`df2`にはなかった北海道や愛知県などの行ができました。そして、`df2`にはない情報はすべて欠損値 (`NA`)となりました。

続いて、`left_join()`ですが、これは先に指定したデータに存在するキーのみで結合を行います ( @fig-handling2_merge_left )。今回は`df2`が先に指定されていますが、`df2`のキーは`df4`のキーの部分集合であるため、`inner_join()`と同じ結果が得られます。

![`left_join()`の仕組み](Figs/Handling2/Merge_Left.png){#fig-handling2_merge_left width="75%" fig-align="center"}

一方、`right_join()`は`left_join()`と逆の関数であり、後に指定したデータに存在するキーを基準に結合を行います ( @fig-handling2_merge_right )。後に指定された`df4`のキーは`df2`のキーを完全に含むので、`full_join()`と同じ結果が得られます。

![`right`_join()`の仕組み](Figs/Handling2/Merge_Right.png){#fig-handling2_merge_right width="75%" fig-align="center"}

これからは`df2`と`df4`を結合することになりますが、この2つのtibbleの大きさが異なります。`df2`は9つの都府県のみであるに対し、`df4`は47都道府県全てのデータが入っているからです。

ここまではキー変数が一つである場合についてのみ考えましたが、複数のキー変数が必要な場合もあります。たとえば、市区町村の人口・面積データと市区町村の投票率データを結合するとします。各自治体に与えられている「[全国地方公共団体コード](https://www.soumu.go.jp/denshijiti/code.html)」が両データに含まれている場合は、このコードをキー変数として使えば問題ありませんが、市区町村名をキー変数として使わざる得ないケースもあるでしょう。しかし、キー変数が複数ある場合もあります。たとえば、府中市は東京都と広島県にありますし、太子町は大阪府と兵庫県にあります。この場合、市区町村名のみでケースをマッチングすると、重複されてマッチングされる恐れがあります。この場合はキー変数を増やすことで対処できます。たとえば、同じ都道府県なら同じ市区町村は存在しないでしょう[^merge2]。キー変数を複数指定する方法は簡単です。たとえば、市区町村名変数が`Munip`、都道府県名変数が`Pref`なら`by = c("Munip", "Pref")`と指定するだけです。

[^merge2]: 行政区も含むなら多くの政令指定都市にある「南区」とか「北区」が重複しますが、ここでは考えないことにしましょう。

最後に、キー変数以外の変数名が重複する場合について考えましょう。これはパネルデータを結合する時によく直面する問題です。同じ回答者に2回の調査を行った場合、回答者のIDでデータを結合することになります。ただし、それぞれのデータにおいて回答者の性別に関する変数が`F1`という名前の場合、どうなるでしょうか。同じデータの同じ名前の変数が複数あると、非常に扱いにくくなります。実際の結果を見てみましょう。

```{r}
Wave1_df <- tibble(ID = c(1, 2, 3, 4, 5),
                   F1 = c(1, 1, 0, 0, 1),
                   F2 = c(18, 77, 37, 50, 41),
                   Q1 = c(1, 5, 2, 2, 3))

Wave2_df <- tibble(ID = c(1, 3, 4, 6, 7),
                   F1 = c(1, 0, 0, 0, 1),
                   F2 = c(18, 37, 50, 20, 62),
                   Q1 = c(1, 2, 2, 5, 4))

full_join(Wave1_df, Wave2_df, by = "ID")
```

それぞれの変数名の後に`.x`と`.y`が付きます。この接尾辞 (suffix)は`suffix`引数を指定することで、分析側からカスタマイズ可能です。たとえば、接尾辞を`_W1`、`_W2`にしたい場合は

```{r}
full_join(Wave1_df, Wave2_df, by = "ID", suffix = c("_W1", "_W2"))
```

のように、データ1とデータ2それぞれの接尾辞を指定するだけです。
