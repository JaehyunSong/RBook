# スクレイピング {#sec-scraping}

```{r scraping-common}
#| include: false
source("_common.R")
```

　まず、本章で使用するパッケージを読み込んでおく。

```{r}
#| label: scraping-setup
pacman::p_load(tidyverse, rvest, gt, gtExtras)
```

## HTML

　我々が普段見るウェブページは主にHTML（**H**yper**T**ext **M**arkup **L**anguage）という言語で記述されている。裏ではPhp、Ruby、Pythonなどが動いているかも知れないが、少なくとも我々がウェブブラウザー（Firefox、Chrome、Safari、Edge等）越しで見る内容はHTML（+CSS、JavaScript、WebAssembly等）で記述されたものだ。（ウェブ）スクレイピングはこのHTMLで記述された表示内容（の一部）を構造化されたデータとして読み込むことである。

　したがって、スクレイピングをするためにはHTMLの基本的な知識が必要だ。一つの画面に表示された内容の中で我々が欲しいものは、全体内容の一部だ。これはスクレイピングを行う際、全体内容の中から取得する箇所を指定する必要があることを意味する。そこで重要なのがタグ（tag）と属性（attribute）、セレクター（selector）だ。

### タグ

　タグは`<タグ名>`と`</タグ名>`で構成され[^tag-pair]、この間に挟まれた内容は予め決まった書式となる。例えば、`<em>R Not for Everyone</em>`は「R Not for Everyone」という文字列に対して`<em>`タグを適用するコードである。`<em>`タグは予めHTMLで用意されているものであり、文字列をイタリック（例：<em>R Not for Everyone</em>）にするものだ。また、`<strong>`タグは太字を意味し、`<strong>R Not for Everyone</strong>`は「<strong>R Not for Everyone</strong>」と出力される。また、段落を意味する`<p>`タグも頻繁に使われる。HTMLには様々なタグが用意されており、詳細なリストは[W3C](https://www.w3.org/TR/2018/SPSD-html5-20180327/index.html#elements-1)などを参照されたい（リンク先はHTML5基準）。

[^tag-pair]: 通常、タグはカッコのように開いたら（`<タグ名>`）閉じる（`</タグ名>`）必要がある。しかし、タグの中には閉じる必要のないものもある。たとえば、改行を意味するタグとして`<br>`があるが、これは`<br>`、または`<br/>`のみで良い。

　タグの中にタグを入れることもできる。以下のコードを見てみよう。

::::{.columns}
:::{.column width=49%}
**HTMLコード**

```{.html code-line-numbers="true"}
<ol>
  <li> 項目1 
    <ul>
      <li> 項目1A </li>
      <li> 項目1B </li>
      <li> 項目1C </li>
    </ul>
  </li>
  <li> 項目2 </li>
  <li> 項目3 </li>
</ol>
```
:::

:::{.column width=2%}
:::

:::{.column width=49%}
**ブラウザー上の出力内容**

1. 項目1
   * 項目1A
   * 項目1B
   * 項目1C
1. 項目2
1. 項目3
:::
::::

　`<ol>`は順序付きリスト（箇条書き）を意味し、一つ一つの項目は`<li>`タグで指定する。以上の例は`<ol>`タグの中に`<li>`タグが入っている入れ子構造だ。また、順序なしリストのタグ`<ul>`は最初の`<li>`の中に入っている。たとえば、「項目1B」は`<ol>` > `<li>` > `<ul>` > `<li>`で定義された内容である。

### 属性

　タグの中には属性といものが定義されている場合がある。タグをプログラミング言語における関数とすれば、属性は引数（argumentとparameter）に該当する。たとえば、画像を貼り付けるタグは`<img>`だ。ちなみに`<img>`はタグを閉じる必要がなく、単体のみ存在するため`<img>`〜`</img>`でなく、`<img>`のみか`<img/>`と記述する。本書では単体で使うタグを区分するために`<img/>`と表記する。この`<img/>`タグだけではどの画像を表示するかが分からない。画像の具体的なパスやURLを指定する必要がある。`<img/>`タグには`src`という属性があり、`src="パス or URL"`と書く。たとえば、<https://www.jaysong.net/RBook/Figs/favicon.png>というURLの画像を表示させるためには`<img src="https://www.jaysong.net/RBook/Figs/favicon.png"/>`と記述する必要がある。

　一つのタグは複数の属性を持つこともできる。`<img/>`タグの場合、画像の幅と高さを`width`と`height`属性で指定することができ、`alt`で代替テキストを指定することもできる。ちなみに属性が不要なタグもあるが、属性を持つことが**できない**タグは存在しない。すべてのタグは`class`や`hidden`、`style`などの属性を持つことができ、このようにすべてのタグで使える属性はグローバル属性（global attributes）と呼ばれる。

### セレクター

　セレクターを理解するためにはCSS（Cascading Style Sheets）の知識が必要であるが、ここでは最低限のことのみ解説する。ウェブスクレイピングは指定したHTMLファイルから特定のタグに囲まれた内容を取得するのが一般的、かつ基本的なやり方だ。たとえば、あるページ上の表を取得するためには表のタグである`<table>`タグで囲まれた内容を取得する。しかし、一つのページ内に複数の`<table>`タグがあればどうだろうか。多くのスクレイピングのパッケージやライブラリはすべてを読み込むが、それはメモリの無駄遣いだ。予め具体的にどの表を取得するかを指定した方が効率的だろう。ここで必要なのがセレクターだ。

　そもそもセレクターが何なのかを知るためには、CSSの話を簡単にしておく必要がある。CSSはHTMLの「見た目」を担当するものであり、通常、HTMLとは別途のファイル（`.css`ファイル）で作成され、HTMLに読み込まれる。`.css`ファイルの内部には「この箇所はこのような見た目にしてくれ」といったものが細かく書かれている。

　まずは以下の簡単な[HTMLページ](Data/scraping/sample00.html){target="_blank"}（`sample00.html`）を確認してみよう。

* [https://www.jaysong.net/RBook/Data/scraping/sample00.html](Data/scraping/sample00.html){target="_blank"}

　変哲もないページであるが、このページのソースコードは以下の通りである。例えば、`<title>`タグで囲まれているテキストはそのページのタイトルとなり、`<h1>`は見出しとなる。いくつかのタグには`id`や`class`といった属性もついている。たとえば、7行目の`<a>`タグには`href`、`id`、`class`の3つの属性がある。

```{.html filename="https://www.jaysong.net/RBook/Data/scraping/sample00.html" code-line-numbers="true"}
<html>
	<head>
		<meta charset="utf-8">
		<title>HTMLの例</title>
	</head>
	<body>
		<h1>第1章：文章</h1>
		<p>『<a href="https://www.jaysong.net/RBook/" id="rbook" class="book-title">私たちのR</a>』は<a href="https://www.jaysong.net/">宋財泫</a>（SONG Jaehyun）と<a href="https://yukiyanai.github.io/">矢内勇生</a>が共同で執筆するRプログラミングの「入門書」である。統計学の本ではない。</p>
		<p>また、本書はデータ分析の手法の解説書でもない。Rを用いたデータ分析については他の本を参照されたい。私たちが専門とする政治学におけるデータ分析については、以下の本を勧める。</p>
		<h1>第2章：箇条書き</h1>
		  <ul>
    		<li>浅野正彦・矢内勇生. 2018. 『<span class="book-title">Rによる計量政治学</span>』オーム社.</li>
    		<li>飯田健. 2013.『<span class="book-title">計量政治分析</span>』共立出版.</li>
  		</ul>
  		<ol>
    		<li>聞いて...</li>
    		<li>感じて...</li>
    		<li>考えて...</li>
  		</ol>
		<h1>第3章：表</h1>
		<h2>数学成績</h2>
		<table class="score" id="math">
			<thead>
				<tr>
					<td>ID</td>
					<td>名前</td>
					<td>成績</td>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td>1</td>
					<td>田中</td>
					<td class="tbl-score">80</td>
				</tr>
				<tr>
					<td>2</td>
					<td>佐藤</td>
					<td class="tbl-score">100</td>
				</tr>
				<tr>
					<td>3</td>
					<td>渡辺</td>
					<td class="tbl-score">75</td>
				</tr>
			</tbody>
		</table>
		<h2>英語成績</h2>
		<table class="score" id="english">
			<thead>
				<tr>
					<td>ID</td>
					<td>名前</td>
					<td>成績</td>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td>1</td>
					<td>田中</td>
					<td class="tbl-score">20</td>
				</tr>
				<tr>
					<td>2</td>
					<td>佐藤</td>
					<td class="tbl-score">100</td>
				</tr>
				<tr>
					<td>3</td>
					<td>渡辺</td>
					<td class="tbl-score">90</td>
				</tr>
			</tbody>
		</table>
	</body>
</html>
```

　続いて、もう一つの[ページ](Data/scraping/sample01.html){target="_blank"}（`sample01.html`）も見てみよう。

* [https://www.jaysong.net/RBook/Data/scraping/sample01.html](Data/scraping/sample01.html){target="_blank"}

　内容的には同じものであるが、見た目がだいぶ異なることが分かるだろう。ソースコードを見ると、一行を除き、`sample00.html`と`sample01.html`のコードは一致していることが分かる。具体的には4行目に`<link/>`タグが追加されているだけだ。この4行目のコードは`style01.css`ファイルを読み込み、本ファイル（`sample01.html`）へ適用するということを意味する。他の内容は`sample00.html`と全く同じだ。つまり、この2つのファイルの見た目が異なるのは`style01.css`の存在が原因であると推測できる。

:::{.panel-tabset}
## `.html`コード

```{.html filename="https://www.jaysong.net/RBook/Data/scraping/sample01.html" code-line-numbers="true"}
<html>
	<head>
		<meta charset="utf-8">
		<link href="sample01.css" rel="stylesheet" type="text/css" media="all"/>
		<title>HTMLの例</title>
	</head>
	<body>
		<h1>第1章：文章</h1>
		<p>『<a href="https://www.jaysong.net/RBook/" id="rbook" class="book-title">私たちのR</a>』は<a href="https://www.jaysong.net/">宋財泫</a>（SONG Jaehyun）と<a href="https://yukiyanai.github.io/">矢内勇生</a>が共同で執筆するRプログラミングの「入門書」である。統計学の本ではない。</p>
		<p>また、本書はデータ分析の手法の解説書でもない。Rを用いたデータ分析については他の本を参照されたい。私たちが専門とする政治学におけるデータ分析については、以下の本を勧める。</p>
		<h1>第2章：箇条書き</h1>
		  <ul>
    		<li>浅野正彦・矢内勇生. 2018. 『<span class="book-title">Rによる計量政治学</span>』オーム社.</li>
    		<li>飯田健. 2013.『<span class="book-title">計量政治分析</span>』共立出版.</li>
  		</ul>
  		<ol>
    		<li>聞いて...</li>
    		<li>感じて...</li>
    		<li>考えて...</li>
  		</ol>
		<h1>第3章：表</h1>
		<h2>数学成績</h2>
		<table class="score" id="math">
			<thead>
				<tr>
					<td>ID</td>
					<td>名前</td>
					<td>成績</td>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td>1</td>
					<td>田中</td>
					<td class="tbl-score">80</td>
				</tr>
				<tr>
					<td>2</td>
					<td>佐藤</td>
					<td class="tbl-score">100</td>
				</tr>
				<tr>
					<td>3</td>
					<td>渡辺</td>
					<td class="tbl-score">75</td>
				</tr>
			</tbody>
		</table>
		<h2>英語成績</h2>
		<table class="score" id="english">
			<thead>
				<tr>
					<td>ID</td>
					<td>名前</td>
					<td>成績</td>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td>1</td>
					<td>田中</td>
					<td class="tbl-score">20</td>
				</tr>
				<tr>
					<td>2</td>
					<td>佐藤</td>
					<td class="tbl-score">100</td>
				</tr>
				<tr>
					<td>3</td>
					<td>渡辺</td>
					<td class="tbl-score">90</td>
				</tr>
			</tbody>
		</table>
	</body>
</html>
```

## `.css`コード

```{.css filename="https://www.jaysong.net/RBook/Data/scraping/sample01.css" code-line-numbers="true"}
h1, h2, h3 {
	font-family: sans-serif;
}
a {
	text-decoration: none;
	color: royalblue;
}
table {
	border-collapse: collapse;
	border: 1px solid;
}
td {
	border-collapse: collapse;
	border: 1px solid;
}
thead {
	text-align: center;
	font-weight: 600;
}
#rbook {
	color: red;
}
.book-title {
	font-weight: 600;
}
.score {
	width: 300px;
}
.tbl-score {
	text-align: right;
}
```
:::

　一つずつ確認していこう。まず、「第1章：文章」や「英語成績」のような見出しが明朝体（serif）からゴジック体（sans-serif）に変わったことが分かる。続いて`sample01.css`の1〜3行目を確認してみよう。

```css
h1, h2, h3 {
	font-family: sans-serif;
}
```

　このCSSの意味は`<h1>`、`<h2>`、`<h3>`タグに囲まれた内容に対し、`{}`内の設定を適用するといういみで、今回はフォント族（font-family）をゴジック（sans-serif）にした。また、リンクの下線が無くなり、文字の色もロイヤルブルーになったが、これも`sample01.css`の4〜7行目で適宜されたものである。このように`タグ名 {}`で特定のタグに対し、スタイルを適用することができ、ここでの`タグ名`はタグに対するセレクターである。このようなタグ名のセレクターは要素型セレクター（type selector）と呼ばれる。

　引き続き、`sample01.html`を見ると『私たちのR』が太字、かつ赤色になっていることが分かる。また、コードの8行目を見ると『私たちのR』の部分が`<a>`タグで囲まれ、
リンク先を意味する`href`属性以外にも、`id`と`class`にそれぞれ`"rbook"`と`"book-title"`の値が指定されていることが分かる。そして、`sample01.css`の20〜25行目に`id`が`rbook`の場合と`class`が`book-title`の場合のスタイルが定義されている。たとえば、idは`#ID名`がセレクターであり（今回は`#rbook`）、赤色が定義されている。クラスは`.クラス名`がセレクターであり（今回は`.book-title`）、文字の太さ（weight）が600になっていることが分かる。ちなみに一つのタグに対して複数のクラスを与えることもできる。この場合、タグ内に`class="クラス名1 クラス名2 クラス名3"`のように半角スペースでクラス名を区切れば良い。

　このようにIDセレクターとクラスセレクターが用意されているが、特定のタグに識別可能な名前を付ける点で、2つの役割は非常に似ている。しかし、IDとクラスには決定的な違いがある。それはIDは一つのページ内において**1回**しか登場できないものの、クラスはこのような制限がないことだ。何回も登場するスタイルであればクラスを使用し、固有の識別子が必要な場合はIDを使う。それでも「クラスを1回だけ使っても良いのでは？」と思う読者もいるだろう。たしかにその通りである。しかし、IDとクラスのもう一つの違いはスタイルが衝突する場合、IDセレクターがクラスセレクターに優先する点にある。たとえば、あるタグが`#A`IDと`.B`クラスを両方持ち、`.css`内部においてそれぞれ文字の太さが600、300に定義されていると、`#A`に指定された太さ600が適用される。スクレイピングにおいてIDとクラスの違いは重要ではないが、念のために述べておく。

　以上で紹介したもの以外にも、セレクターは多数用意されている。たとえば、文章全体にスタイルを適用したい場合のセレクターは`*`であり、全称セレクター（universal selector）と呼ばれる。また、特定の属性を持つタグに対してスタイルを適用できる。たとえば、`<img>`タグすべてでなく、`alt`属性を持つ`<img>`タグのみにスタイルを適用する場合のセレクターは`img[alt]`のように`タグ名[属性名]`のように記述する。また、`a[href="http://www.jaysong.net"]`のように`href`属性の値が`"http://www.jaysong.net"`と一致する`<a>`タグを選択することもできる[^attr-selector]。

[^attr-selector]: 属性セレクターはいくつかのバリエーションがある。たとえば、`a[href$=".net"]`は`href`の値が`".net"`で**終わる**`<a>`タグ、`a[href^="www"]`は`href`の値が`"www"`で**始まる**`<a>`タグ、`a[href~="jaysong"]`は`href`の値に`"jaysong"`が**含まれていない**`<a>`タグ、`a[href*="rbook"]`は`href`の値に`"rbook"`が**含まれている**`<a>`タグを意味する。

:::{.callout-note}
## CSSセレクターとXPath

　以上ではCSSセレクターの書き方について紹介したが、実はHTML内の要素を指定するもう一つの記述方法があり、それがXPathというものだ。たとえば、`<img>`タグの場合、CSSは`img`、XPathでは`//img`と記述する。また、クラスはCSSだと`.クラス名`、XPathだと`//*[contains(@class,"クラス名")]`となり、IDはそれぞれ`#ID名`、`//*[@id="ID名"]`となる。通常、CSSセレクターの方がXPathより簡潔なので、より幅広く使われる。しかし、XPathではCSSセレクターでは指定できない要素まで指定できるなど、機能面ではより強力だ。また、規則的なページ構造を持たないページのスクレイピングは[Selenium](https://www.selenium.dev/ja/documentation/)というものを使う場面が多いが、SeleniumではCSSセレクターよりXPathの方が幅広く使われる。本章で紹介する{rvest}パッケージはいずれの書き方にも対応しているが、引数を指定しない場合は第2引数であるCSSセレクターの書き方となる。XPathを使う場合は`xpath = "XPath ID"`のように仮引数名を指定する必要がある。
:::

### セレクターの確認

　CSSを勉強する場合のセレクターの話はもっと長くなるが、スクレイピング**入門**レベルであれば、タグ、ID、クラス、属性セレクターだけでも問題ない[^selector-advanced]。つまり、自分がスクレイピングしたい内容のタグ、ID、クラス、属性を知るだけで十分だ。これを調べるにはHTMLソースコードを読む必要はあるが、最近のHTMLページは数百〜数千行のコードで構成されているため、すべてを精査することは現実的でない。最近のウェブブラウザーには開発者専用のメニューが用意されており、これを活用すると素早く必要な内容のセレクターを調べることができる。しかし、ブラウザーごとに開発者メニューの開き方が異なる。以下では[statcounter](https://gs.statcounter.com/browser-market-share)基準、代表的な4つのブラウザーの例を紹介する。

[^selector-advanced]: もっと深く入ると、セレクターの親子関係など様々な書き方がある。

:::{.panel-tabset}
## Firefox

右上の「≡」>その他のツール>ウェブ開発ツール

![](Figs/Scraping/devtools_firefox.png)

## Chrome

右上の「⋮」>その他のツール>デベロッパーツール

![](Figs/Scraping/devtools_chrome.jpg)

## Safari

開発 > Webインスペクタを接続

* 開発メニューがない場合は環境設定の「詳細」タブの「メニューバーに"開発"メニューを表示」にチェックを入れる必要がある。

![](Figs/Scraping/devtools_safari.jpg)

## Edge

右上の「…」>その他のツール>開発者ツール

![](Figs/Scraping/devtools_edge.png)
:::

　ここからは筆者（宋）が使用しているFirefox基準で説明するが、どのブラウザーでも使い方は大きく変わらない。以下は『私たちのR』の初期ページからウェブ開発メニューを開いたものである。

![](Figs/Scraping/find_selector00.png)

　このページのコードは2023年6月現在、1148行である。ここで、画面右にある『私たちのR』のカーバー（仮）のタグ、クラス、ID、属性、親タグなどを調べてみよう。まず、ウェブ開発メニューの左上にある![](Figs/Scraping/find_selector01.png){fig-width=10px}ボタンをクリックする。これはページ内の要素を選択し、その要素のコードなどを表示してくれる機能である。このボタンのアイコンはブラウザーごとにことなるが、開発者メニューの左上か右上に位置する。

　続いて、調べたい要素を選択する。マウスカーソルを要素の上に乗せるとハイライトされるため、分かりやすい。調べたい要素がハイライトされたらそのままクリックすると、開発者メニューに当該箇所のソースコードが表示される。

![](Figs/Scraping/find_selector02.png)

　以下は当該箇所のコードの一部を抜粋したものだ。

```{.html code-line-numbers="true"}
<section id="紹介" class="level1 unnumbered">
  <h1 class="unnumbered">紹介</h1>
  <p>
    <img src="Figs/Cover.png" title="私たちのR" class="quarto-cover-image img-fluid">
  </p>
```

　当該箇所のタグは`<img>`である。クラスは`quarto-cover-image`と`img-fluid`、2つだ。他にも`src`と`title`という属性を持ち、それぞれ`"Figs/Cover.png"`と`"私たちのR"`という値が割り当てられている。他にもこの`<img>`タグの親タグは`<p>`であり、その親タグは`<section>`タグだということが分かる。ここでこの図の情報が必要な場合、セレクターは`img`十分だろうか。答えはNoだ。このページにはこの画像以外にもイケメン著者たちの写真もある。`img`だけだとどの図なのかが分からない。この図を特定するためには更に情報が必要だ。

　たとえば、`<img>`の`title`属性に注目しても良いだろう。イケメン著者たちの画像は`title`属性を持たない（各自確認してみよう）が、カーバー（仮）には`title="私たちのR"`がある。これを利用すると`img[title="私たちのR"]`といったセレクターも有効だろう。もう一つは親のタグ、ID、クラスなどを利用する方法だ。カーバー（仮）の親タグの一つは`<section>`であり、`"紹介"`というIDが指定されている。IDはこのページに1回しか登場しないものであるため、これは使えるかも知れない。このページ内の2つの画像のコードを簡単に示すと以下の通りだ。

```{.html code-line-numbers="true"}
<section id="紹介" class="level1 unnumbered">
  <img src="Figs/Cover.png" title="私たちのR" class="quarto-cover-image img-fluid">
</section>
<section id="著者紹介" class="level1 unnumbered">
  <img src="Figs/Authors/SongYanai.jpg" class="img-fluid figure-img" width="350">
</section>
```

　どの画像も親タグは`<section>`であるが、異なるIDを持つ。つまり、異なる親を持つ。カーバー（仮）は`id="紹介"`、イケメン著者は`id="著者紹介"`の`<section>`親を持つ。この場合、（1）IDが`"著者"`の要素を選択し（`#紹介`）、（2）`<img>`タグを選択する（`img`）といった手順で、欲しい内容が抽出できる。本章では基本的に、このような多段階の抽出方法を採用する。より洗練された書き方で効率的なスクレイピングもできるが、入門レベルだとこのようなやり方でも問題ないだろう。それではRにおけるスクレイピングの定番パッケージ、{rvest}の簡単な使い方を見てみよう。

## {rvest}の使い方

　実際のウェブスクレイピングをやってみる前に、{rvest}パッケージを使って[実習用のページ](https://www.jaysong.net/RBook/Data/scraping/sample01.html)（`sample01.html`）の内容を取得してみよう。

* テキスト/表の取得
   * `html_elment()`、`html_elments()`で特定のタグやクラス、IDを抽出
   * `html_text()`、`html_text2()`や`html_table()`で抽出
   * 画像の場合、`<img>`タグの`src`属性を抽出する必要があるため、`html_attr()`

　まずは、`read_html()`関数を使用し、スクレイピングするHTMLファイルそのものを読み込んでおく必要がある。引数はHTMLファイルのURL、もしくはパスだけで問題ないが、Shift-JISやEUC-KRといった邪悪なロケールで作成されたページであれば、`encoding`引数が必要となる。サンプルページはUTF-8であるため、URLのみで問題ない。

```{r}
#| label: scraping-rvest-read-1
my_html <- read_html("https://www.jaysong.net/RBook/Data/scraping/sample01.html")
```

```{r}
#| label: scraping-rvest-read-2
my_html
```

　中身は簡単にしか確認できないが、そもそもRでコードをすべて表示する必要もないので問題ないだろう。HTMLのソースコード全体が見たい場合はウェブブラウザーから確認しよう。ここでは問題なくHTMLファイルが読み込まれていることだけを確認すれば良い。

　それではこの`my_html`からいくつかの要素を抽出してみよう。まずは、`タグ名`セレクターを使用し、特定のタグだけを抽出する。ここで使用する関数は`html_elements()`だ[^html_elements]。たとえば、ハイパーリンクを意味する`<a>`タグが使用された箇所すべてを読み組むには`html_elemtns(HTMLオブジェクト名, "a")`で良い。HTMLオブジェクト名（今回は`my_html`）は第1引数だから、パイプ演算子を使用しよう。

[^html_elements]: 似たような名前の`html_element()`がある。ほぼ同じものであるが、`html_element()`は1ページ内に当該セレクターが複数回ある場合、最初のものだけを抽出する。一方、`html_elements()`はすべての要素を抽出する。

```{r}
#| label: scraping-rvest-elements-01
my_html |> 
  html_elements("a")
```

　しかし、通常、これらの内容すべてが必要になるケースは稀だろう。普通、`<a>`と`</a>`に囲まれたテキストの内容や、リンク先のURLのリストが欲しいだろう。`html_elements()`で指定したセレクター内のテキストを抽出する場合は、`html_text()`を使用する[^html_text2]。

[^html_text2]: 似たような関数として`html_text2()`があるが、これは前後の不要なスペースやタブ、改行などを除外した上でテキストを取得する関数だ。

```{r}
#| label: scraping-rvest-elements-02
my_html |> 
  html_elements("a") |> 
  html_text()
```

　`<a>`と`</a>`に囲まれたテキストの内容でなく、リンク先のURLを抽出することもできる。`<a>`タグのリンク先は`href`属性で指定するため、`href`属性の値を抽出すれば良い。特定の属性の値を取得する関数は`html_attr()`であり、引数として属性名を指定すれば良い。

```{r}
#| label: scraping-rvest-elements-03
my_html |> 
  html_elements("a") |> 
  html_attr("href")
```

 続いて、箇条書きの要素を抽出してみよう。今回は2本の書籍リストが対象だ。箇条書きの内容は`<li>`タグで記述されるので、`html_elements("li")`で`<li>`タグの内容を取得してみよう。

```{r}
#| label: scraping-rvest-elements-04
my_html |> 
  html_elements("li")
```

　書籍リストだけでなく、謎の言葉も取得される。実際、サンプルページには2つの箇条書きがあり、書籍は順序なしの箇条書き（`<ul>`）、謎の言葉は順序付き箇条書き（`<ol>`）である。したがって、まず、`<ul>`タグを抽出し、そこから`<li>`を抽出すれば良い。

```{r}
#| label: scraping-rvest-elements-05
my_html |> 
  html_elements("ul") |> 
  html_elements("li")
```

　ちなみに`html_elements()`が続く場合は引数を`"タグ名 > タグ名"`にしても同じ結果が得られる。

```{r}
#| label: scraping-rvest-elements-06
my_html |> 
  html_elements("ul > li")
```

　ここから更にテキストのみ抽出する場合は`html_text()`を使えば良い。

```{r}
#| label: scraping-rvest-elements-07
my_html |> 
  html_elements("ul > li") |> 
  html_text()
```

　`html_elements()`にはタグ名以外のセレクターも使える。たとえば、クラス（`.クラス名`）やID（`#ID名`）の指定もできる。サンプルページには書籍名が3回登場し、それらは`<span>`タグに囲まれている。`<span>`タグそのものは機能を持たないが、文章の一部などにIDやクラスを割り当てる際によく使われる[^span-div]。今回の例だと、書籍名はクラスが`book-title`の`<span>`タグで囲まれている。したがって、書籍名を抽出する時には`html_elements(".book-title")`

[^span-div]: 似たようなものとして`<div>`がある。`<span>`は主に文章の一部（=インライン; inline）に対して使用することで当該箇所にクラスやIDを割り当てる。一方、`<div>`は**領域**に対してクラスやIDを割り当てる際に使用する。

```{r}
#| label: scraping-rvest-elements-08
my_html |> 
  html_elements(".book-title")
```

　このように`book-title`クラスのタグと、その内容が全て抽出される。ここからテキストを抽出する場合は`html_text()`を使えば良い。

```{r}
#| label: scraping-rvest-elements-09
my_html |> 
  html_elements(".book-title") |> 
  html_text()
```

　スクレイピングで最も需要の高いものは表だろう。表の場合、HTMLでは`<table>`タグで記述されるが、{rvest}は表を取得し、tibble形式で返す`html_table()`関数が用意されている。これまで使ってきた`html_elements()`関数はあっても良いが、なくても問題ない。サンプルページには2つの表があるが、HTMLオブジェクトをそのまま`html_table()`に渡すと表が抽出される。

```{r}
#| label: scraping-rvest-table-01
my_html |> 
  html_table()
```

　長さ2のリストが出力され、それぞれ表が格納されている。今回のように、表のヘッダー（1行目）が中身として出力される場合もあるが、このような場合は`header = TRUE`を指定すると、1行目が変数名となる。

```{r}
#| label: scraping-rvest-table-02
my_tables <- my_html |> 
  html_table(header = TRUE)

my_tables
```

　この2つの表を`bind_rows()`を使って結合することもできる。まず、`names()`関数を使って、リストの各要素に名前を割り当てる。

```{r}
#| label: scraping-rvest-table-03
names(my_tables) <- c("数学", "英語")

my_tables
```

　続いて、`bind_rows()`関数に表のリストを入れ、`.id`変数で2つの表を識別する値が格納される列名を指定する。`bind_rows()`の詳細は第[-@sec-handling3-merge]章を参照されたい。

```{r}
#| label: scraping-rvest-table-04
my_table <- bind_rows(my_tables, .id = "科目")

my_table
```

　すべての表ではない、英語成績の表だけを抽出する場合はどうすれば良いだろうか。今回は表が2つしかなく、2番目の表が英語成績ということが分かっているので`my_table[[2]]`のような書き方でも問題ない。しかし、表が数百個ある場合は、何番目の表かを数えるのも簡単ではない。幸い、今回はそれぞれの表に`math`と`english`といったIDが割り当てられている。このセレクターを使えば、IDが`"english"`の表を選択することもできよう。`html_element()`関数の引数として`"#english"`を指定し、そこから`html_table()`を実行すると英語成績の表だけが抽出される。

```{r}
#| label: scraping-rvest-table-05
my_html |> 
  html_element("#english") |> 
  html_table(header = TRUE)
```

　ここで一つ注意事項があるが、`html_elements()`でなく、`html_element()`を使う点だ。IDが1ページに1つしか存在しないため、`html_element()`を使った方が楽である。`html_elements()`を使っても良いが、返ってくるのはtibbleでなく、長さ1のリストになるので、更に`[[1]]`などでリストから表を取り出す必要がある。

## 実践

### テキスト

![](Figs/Scraping/rn4e_index.png)

```{r}
#| label: scraping-text-1
rbook_html <- read_html("https://www.jaysong.net/RBook/")
rbook_html
```

　ここからリンクを意味する`<a>`を抽出すれば良いが、このページには各章へのリンク以外にも多くの`<a>`タグが存在する。したがって、範囲を絞る必要があるが、本ページの「進行状況」の領域に限定すれば良いだろう。開発者メニューを見ると「進捗状況」の見出しには`進捗状況`というIDが振られているため、まずはIDセレクターで要素を絞ってから`<a>`タグを抽出する。

```{r}
#| label: scraping-text-2
rbook_html |> 
  html_element("#進捗状況") |> 
  html_elements("a")
```

　ここから更に`href`属性に割り当てられた値（ここではURL）のみを抽出する必要がある。ここで`html_attr()`関数を使用する。抽出したい属性名を引数として指定するだけだ。

```{r}
#| label: scraping-text-3
rbook_urls <- rbook_html |> 
  html_element("#進捗状況") |> 
  html_elements("a") |> 
  html_attr("href")

rbook_urls
```

```{r}
#| label: scraping-text-4
rbook_html |> 
  html_element("#進捗状況") |> 
  html_elements("li") |> 
  html_text()
```

　概ね問題はなさそうに見えるが、章だけでなく、部（`"第1部: Rの導入\n第1章: R?\n第2章: Rのインストール\n第3章: IDEの導入\n第4章: 分析環境のカスタマイズ\n第5章: Rパッケージ\n"`など）が書かれた`<li>`タグまで抽出されてしまった。スクレイピングする内容が今回のように少ないのであれば、自分で一つ一つ消しても良いが、ここではすべて自動化しよう。

　まず、開発者メニューを開き、章の箇所（どの章でも良い）を選択する。以下は「第1章: R?」とその前後のコードである。

```{.html code-line-numbers="true"}
<section id="進捗状況" class="level2">
<h2 class="anchored" data-anchor-id="進捗状況">進捗状況</h2>
<p>章立ては未定。著者が書きたいものから書く予定 (全部で30~35章くらいになる見込み)。</p>
<ul>
  <li>第1部: Rの導入
    <ul>
      <li>第<a href="aboutr.html" class="quarto-xref"><span>1</span></a>章: R?</li>
      <li>第<a href="installation.html" class="quarto-xref"><span>2</span></a>章: Rのインストール</li>
      <li>第<a href="ide.html" class="quarto-xref"><span>3</span></a>章: IDEの導入</li>
      <li>第<a href="r_customize.html" class="quarto-xref"><span>4</span></a>章: 分析環境のカスタマイズ</li>
      <li>第<a href="packages.html" class="quarto-xref"><span>5</span></a>章: Rパッケージ</li>
    </ul>
  </li>
  <li>第2部: Rの基礎
```

　「第1章: R?」は上記のコードの7行目にある。また、その親タグは`<ul>`、そしてそれの親タグは`<li>`、そして`<ul>`もある。これは第2章も、第30章も同じだ。つまり、IDが`進捗状況`の内容（`#進捗状況`）の中、`<ul>`の中の`<li>`の中の`<ul>`の中の`<li>`の箇所を取り出せば良い。セレクターは`"ul li ul li"`と記述する[^descendant]。

[^descendant]: 今回の場合、`"ul > li > ul > li"`のような書き方もできる。空白で区切る方法を子孫結合子（descendant combinator）、`>`で区切る方法を子結合子（child combinator）と呼ぶ。タグの構造が`<a><b><c></c></b></a>`の構造になっている場合、`<c>`タグを指定するケースを考えてみよう。子孫結合子だと`"a c"`のように表記することができるが、`<a>`が持つ全ての子孫から`<c>`タグを探索することになる。つまり、`<a>`の子だけでなく、孫、曾孫（やそれ以上）までも探索の対象となる。一方、子結合子は自分の子のみ探索することになるため、`"a > b > c"`のように孫を指定するためには、まずは自分の子を経由する必要がある。

```{r}
#| label: scraping-text-5
rbook_titles <- rbook_html |> 
  html_element("#進捗状況") |> 
  html_elements("ul li ul li") |> 
  html_text()

rbook_titles
```

　今回は章の文字列だけ取得できた。それでは章のタイトルのベクトル（`rbook_titles`）とURL（`rbook_urls`）を一つの表としてまとめてみよう。

```{r}
#| label: scraping-text-6
rbook_df <- tibble(Titles = rbook_titles,
                   URL    = rbook_urls)

print(rbook_df, n = Inf)
```

　続いて、`separate()`関数を使って章とタイトルを`": "`文字列を基準に別の列として分割し、それぞれ`Section`と`Title`という列とする。ただし、30章以降は付録であるため、`": "`は存在しない。したがって、章かタイトルどちらかは欠損値となるが、今回は左側を欠損値として埋めたいので`fill = "left"`を追加する。さらに、URLも完全なURLにする。まず、付録のURLが何故か`"./"`で始まるようになっているので、`str_remove()`を使って除去する。そうすればファイル名だけが残り、後はこれらのファイル名の前に`"https://www.jaysong.net/RBook/"`を付けるだけだ。

```{r}
#| label: scraping-text-7
rbook_df <- rbook_df |>   
  separate(col  = Titles,
           into = c("Section", "Title"),
           sep  = ": ",
           fill = "left") |> 
  mutate(URL = str_remove(URL, "^\\.\\/"),
         URL = paste0("https://www.jaysong.net/RBook/", URL))

print(rbook_df, n = Inf)
```

　最後に`rbook_df`を{gt}パッケージを使って表にする。`sub_missing()`関数を使って、欠損値の箇所を`"付録"`に置換し、`fmt_url()`を使って、URL列をリンクボタン化する。

```{r}
#| label: scraping-text-8
rbook_df |>   
  gt() |> 
  cols_label("Section" = "章",
             "Title"   = "タイトル",
             "URL"     = "リンク") |> 
  sub_missing(columns = Section, missing_text = "付録") |> 
  fmt_url(columns = URL, as_button = TRUE, label = "Link")
```

### 表

　まずは簡単な例から始めよう。ここでは英語版Wikipediaの[世界報道自由度ランキング](https://en.wikipedia.org/wiki/World_Press_Freedom_Index)の表をスクレイピングする。長いURLは別途のオブジェクトに格納しておくと、コードが簡潔になるだけでなく、コードのリサイクルも簡単になる。ここでは読み込んだHTMLファイルを`pfi_html`という名のオブジェクトとして格納しておく。

```{r}
#| label: scraping-hdi-1
url <- "https://en.wikipedia.org/wiki/World_Press_Freedom_Index"

pfi_html <- read_html(url)

pfi_html
```

　続いて、`html_table()`を使用し、`pfi_html`から表（`<table>`タグ）を取得し、`pfi_tbls`オブジェクトに格納する。

```{r}
#| label: scraping-hdi-2
pfi_tbls <- pfi_html |> 
  html_table()

pfi_tbls
```

　当該ページには2つの表があり、我々が欲しい表はリストの1番目の要素である。`pfi_tbls`の1番目の要素のみを抽出し、更に`2023[5]`のような列名を`Year2023`のように修正し、`pfi_df`に格納する。

```{r}
#| label: scraping-hdi-3
pfi_df <- pfi_tbls[[1]] |> 
  rename("Year2023" = "2023[5]",
         "Year2022" = "2022[6]",
         "Year2021" = "2021[7]",
         "Year2020" = "2020[8]",
         "Year2019" = "2019[9]")

pfi_df
```

　`pfi_df`を見ると、`Country`以外の列は世界報道自由度**指数**以外にもカッコ内に**順位**が書かれていることが分かる。具体的には`(順位)指数`の構造になっている。`str_remove()`関数を使って、ここから`(順位)`の箇所を削除しよう。「`(`で始まり（=`^\\(`）、一つ以上の数字が並び（=`([0-9]+)`）、その後に出てくる`)`まで（=`\\)`）」の箇所が削除対象であるため、正規表現は`^\\(([0-9]+)\\)`となる。正規表現の詳細は第[-@sec-string]章を参照されたい。抽出が終わっても、まだcharacter型のままなので、`as.numeric()`関数でnumeric型に変換する。

```{r}
#| label: scraping-hdi-4
pfi_df <- pfi_df |> 
  mutate(across(Year2023:Year2019, ~str_remove(.x, "^\\(([0-9]+)\\)")),
         across(Year2023:Year2019, as.numeric))

pfi_df
```

　これで表の抽出が終わった。すべての表を出力しても良いが、かなり長くなるため東アジア地域のみに限定して`pfi_df`を{gt}パッケージで出力し、指数の値ごとに色分けをする。セルの色塗りについては第[-@sec-gt-highlight]章を参照されたい。

```{r}
#| label: scraping-hdi-5
pfi_df |> 
  filter(Country %in% c("Japan", "South Korea", "North Korea", "China",
                        "Hong Kong", "Taiwan", "Mongolia")) |> 
  gt() |> 
  cols_label("Year2023" = "2023",
             "Year2022" = "2022",
             "Year2021" = "2021",
             "Year2020" = "2020",
             "Year2019" = "2019") |> 
  data_color(columns = Year2023:Year2019,
             palette = "ggsci::blue_material")
```

### 表（複数ページ）

　続いて、複数のHTMLページから表を取得してみよう。今回はJリーグ（1部から3部まで）の順位表が対象である。

* J1: <https://www.jleague.jp/standings/j1/>
* J2: <https://www.jleague.jp/standings/j2/>
* J3: <https://www.jleague.jp/standings/j3/>

　3つのページを見ると、ページの構造は一致することが分かる。また、表を意味する`<table>`タグを見ると、いずれも`scoreTable01`クラスが付与されていること分かる（各自、開発者メニューから確認してみよう）。したがって、同じ作業を3つのページに対して繰り返すだけで良いだろう。まずは、J1リーグだけ試してみよう。まず、HTMLファイルを読み込み、`html_element(".scoreTable01")`で`scoreTalbe01`クラスの要素を抽出し、そこから`html_table()`で表を抽出してみよう。

```{r}
#| label: scraping-j-1
j1_url  <- "https://www.jleague.jp/standings/j1"
j1_html <- read_html(j1_url)

j1_html |> 
  html_element(".scoreTable01") |> 
  html_table()
```

　今回も`header = TRUE`を入れてヘッダーを指定する必要があると考えられる。また、取得する列は`順位`から`得失点`までなので、必要な列だけを抽出してみよう。

```{r}
#| label: scraping-j-2
j1_html |> 
  html_element(".scoreTable01") |> 
  html_table(header = TRUE) |> 
  select(順位:得失点)
```

　以上の作業を3つのページに対して繰り返せば良い。まずは、3つのURLが格納されたcharacter型ベクトルを作成しよう。

```{r}
#| label: scraping-j-3
j_urls <- paste0("https://www.jleague.jp/standings/j", 1:3)
j_urls
```

　また、取得した3つの表を格納する空のリストを作成する。

```{r}
#| label: scraping-j-4
tbl_list <- list()
```

　続いて`for()`を使用し、これらのスクレイピングを繰り返す。`seq_along(j_urls)`は`1:length(j_urls)`と同じ意味だ。

```{r}
#| label: scraping-j-5
for (i in seq_along(j_urls)) {
  # j_urlsのi番目のURLからHTMLファイルを読み込み、temp_htmlに格納
  temp_html <- read_html(j_urls[i])
  # 表を取得し、temp_tblに格納
  temp_tbl  <- temp_html |> 
    html_element(".scoreTable01") |> 
    html_table(header = TRUE) |> 
    select(順位:得失点)
  
  # tbl_listにtemp_tblを格納する。
  # tbl_listの名前はJ1、J2、J3になるようにする。
  tbl_list[[paste0("J", i)]] <- temp_tbl
  
  # 1つの表を取得したら1秒休む
  Sys.sleep(1)
}

tbl_list
```

　`bind_rows()`を使ってリスト内の3つの表を結合し、{gt}パッケージで表示する。

```{r}
#| label: scraping-j-6
j_df <- bind_rows(tbl_list, .id = "リーグ")

j_df |> 
  gt()
```

　クラブ名が「FC矢内FC矢内」のようになっていることが分かる。元のページを見ると、チームのエンブレムが表示されるが、これらの画像には代替テキスト（`alt`）が付与されており、スクレイピングのように画像が読み込めない場合は、その代替テキストが取得されるのが原因である。

　解決方法はいくつかあるが、ここでは事後的な解決方法を採用する。具体的には`クラブ名`列内の文字列を半分だけ残せば良い。たとえば、「FC矢内FC矢内」は8文字であるため、1番目の文字から8文字の半分である4文字まで残す。文字列から一部を取り出す関数は`str_sub()`であり、`str_sub(文字列, スタート位置, 終了位置)`を指定する。スタート位置は1固定であり、終了位置は文字列の長さ（`nchar(文字列)`）の半分だから`nchar(文字列) / 2`で良い。

　処理が終わったら、`リーグ`列でグルーピングして表として出力する。

```{r}
#| label: scraping-j-7
j_df |> 
  mutate(クラブ名 = str_sub(クラブ名, 1, nchar(クラブ名) / 2)) |> 
  group_by(リーグ) |> 
  gt()
```

### 表以外の内容

　続いて、表のように見えて表ではないものをスクレイピングしてみよう。[OpenCritic](https://opencritic.com/)というゲームの口コミサイトから2021年発売されたPCゲームのランキングを取得してみよう。対象ページは以下のURLだ。

* <https://opencritic.com/browse/all/2021>

![](Figs/Scraping/oc_index.png)

　ここには上位20位までのゲームのリストがあり、20行[^20-rows]6列[^6-cols]の**表**のように見える。まず、このページを読み込んだものを`oc_html`に格納する。

[^20-rows]: 20ゲーム

[^6-cols]: 順位、スコア、ティア（Tier）の画像、ゲーム名、機種、発売日

```{r}
#| label: scraping-oc-single-read-1
oc_html <- read_html("https://opencritic.com/browse/all/2021")
oc_html
```

　続いて`oc_html`から`html_table()`を使用し、表を抽出してみよう。

```{r}
#| label: scraping-oc-single-read-2
oc_html |> 
  html_table()
```

　どうみても表があるように見えるものの、`html_table()`から表は抽出されていない。実はこのページの順位表、`<table>`タグで記述された表ではない。100個以上の`<div>`要素を表のように並べたものである。開発者メニューからこの表のようなもののコードを確認してみよう。

```{.html code-line-numbers="true"}
<div _ngcontent-serverapp-c79="" class="desktop-game-display">
	<div _ngcontent-serverapp-c79="">
		<div _ngcontent-serverapp-c79="" class="row no-gutters py-2 game-row align-items-center">
			<div _ngcontent-serverapp-c79="" class="rank"> 1. </div>
			<div _ngcontent-serverapp-c79="" class="score col-auto"> 93 </div>
			<div _ngcontent-serverapp-c79="" class="tier col-auto"><app-tier-display _ngcontent-serverapp-c79="" display="man" _nghost-serverapp-c65=""><img _ngcontent-serverapp-c65="" src="//img.opencritic.com/mighty-man/mighty-man.png" alt="Mighty" width="45" height="42"></app-tier-display></div>
			<div _ngcontent-serverapp-c79="" class="game-name col"><a _ngcontent-serverapp-c79="" href="/game/11626/final-fantasy-xiv-endwalker">Final Fantasy XIV: Endwalker</a></div>
			<div _ngcontent-serverapp-c79="" class="platforms col-auto"> PC, PS5, PS4 </div>
			<div _ngcontent-serverapp-c79="" class="first-release-date col-auto"><span _ngcontent-serverapp-c79="">Dec 7</span></div>
		</div>
		<div _ngcontent-serverapp-c79="" class="row no-gutters py-2 game-row align-items-center">
			<div _ngcontent-serverapp-c79="" class="rank"> 2. </div>
			<div _ngcontent-serverapp-c79="" class="score col-auto"> 92 </div>
			...
```

　数百の`<div>`タグが入れ子構造でなっていることが分かる。この`<div>`タグそのものに特別な機能はない。`<span>`が文中の文章に対してクラスやIDを割り当てるために使われるということは既に説明したが、`<div>`もほぼ同じ役割をする。違いがあれば、`<span>`は**文中の文章**が対象であり、`<div>`は**領域**が対象とすることだ。この構造を実際のページに対応すると以下のようになる。

![](Figs/Scraping/oc_structure.png)

　まず、この順位表全体はクラス名が`desktop-game-display`の`<div>`タグに囲まれている。この中に、各行は`row`、`no-gutters`、`py-2`、`game-row`、`align-items-center`計5つのクラスを持つ`<div>`タグで囲まれている。そしてこの中に順位（クラス名：`rank`）やスコア（クラス名：`score`、`col-auto`）、ティア（クラス名：`tier`、`col-auto`）の`<div>`があり、それぞれのセルを構成している。

　ここからランクを取り出してみよう。まずはクラスが`desktop-game-display`である`<div>`タグに限定するために、`html_element("div.desktop-game-display")`で当該`<div>`のみを残し、`oc_html`を上書きする。このようにクラス名とタグを同時に指定する場合のセレクターは`タグ名.クラス名`と表記する。

```{r}
#| label: scraping-oc-single-3
oc_html <- oc_html |> 
  html_element("div.desktop-game-display")
oc_html
```

　続いて、`oc_html`からクラス名が`rank`の`<div>`タグを抽出する。今回は抽出対象が20個あるため、`html_element()`でなく、`html_elements()`を使用する。

```{r}
#| label: scraping-oc-single-4
oc_html |> 
  html_elements("div.rank")
```

　さらにここから`html_text()`を使用してテキストのみ抽出してみよう。

```{r}
#| label: scraping-oc-single-5
oc_html |> 
  html_elements("div.rank") |> 
  html_text()
```

　前後の空白が気になる。この場合、`html_text2()`を使うと無駄な空白や改行、タブなどを除去することができる。

```{r}
#| label: scraping-oc-single-6
oc_html |> 
  html_elements("div.rank") |> 
  html_text2()
```

　これでランキングの抽出ができた。まだ数字の後に付いている点が気になるが、こちらは後ほどまとめて除去しよう。つづいて、ティアーの画像URLを取得してみよう。既にクラスが`desktop-game-display`である`<div>`タグに限定された`oc_html`からクラス名が`tier`の`<div>`を選択し、さらにそこから`<img>`タグを抽出し、画像のURLが指定されている`"src"`**属性**を抽出してみよう。

```{r}
#| label: scraping-oc-single-7
oc_html |> 
  html_elements("div.tier img") |> 
  html_attr("src")
```

　20個の画像が抽出された。URLなのに`"//"`で始まるのが不自然だと感じるかも知れないが、`"//"`で始まるURLは`"https://"`や`http://`に勝手に認識されるので問題ない。あとはこれらの作業を他のセル（ゲーム名、スコアなど）にも適用し、一つの表としてまとめるだけだ。`oc_df`という名のオブジェクトとして格納しよう。

```{r}
#| label: scraping-oc-single-8
oc_df <- tibble(Rank      = html_elements(oc_html, "div.rank") |> 
                  html_text2(),
                Tier      = html_elements(oc_html, "div.tier img") |> 
                  html_attr("src"),
                Score     = html_elements(oc_html, "div.score") |> 
                  html_text2(),
                Name      = html_elements(oc_html, "div.game-name") |>
                  html_text2(),
                Platforms = html_elements(oc_html, "div.platforms") |>
                  html_text2(),
                Date      = html_elements(oc_html, "div.first-release-date") |>
                  html_text2())

oc_df
```

　後はこの表の細かいところを修正しよう。たとえば、`Rank`列から`"."`を除去し、numeric型に変換したり、`Date`列をdate型に変換する。

```{r}
#| label: scraping-oc-single-9
oc_df <- oc_df |> 
  mutate(Rank  = str_remove(Rank, "\\."), # <1>
         Rank  = as.numeric(Rank),        # <2>
         Score = as.numeric(Score),       # <3>
         Date  = paste0(Date, ", 2021"),  # <4>
         Date  = mdy(Date))               # <5>

oc_df
```

1. `Rank`列から`.`を除去する。
2. `Rank`列をnumeric型に変換する。
3. `Score`列をnumeric型に変換する。
4. `Date`列の後に`", 2021"`を付ける。
4. `Date`列がmonth-day-yearで記述されていることを伝え、date型に変換する。

　最後に`oc_df`を{gt}パッケージを使って表として出力してみよう。

```{r}
#| label: scraping-oc-single-10
oc_df |> 
  gt() |> 
  gt_img_rows(Tier) |> 
  cols_label("Date" = "First release date")
```

<!---

```r
#| label: scraping-oc-single-11
oc_df |> 
  separate(Platforms,
           into = paste0("P", 1:6),
           sep  = ", ",
           fill = "right") |> 
  pivot_longer(cols      = P1:P6,
               names_to  = "temp",
               values_to = "Platform") |> 
  drop_na(Platform) |> 
  select(!temp) |> 
  mutate(Platform = factor(Platform, levels = c("PC", "PS4", "PS5",
                                                "XB1", "XBXS", "Switch",
                                                "Oculus"))) |> 
  arrange(Rank, Platform) |> 
  mutate(Platform = as.character(Platform),
         Platform = recode(Platform,
                           "PC"     = "Figs/Scraping/icon_pc.svg",
                           "PS5"    = "Figs/Scraping/icon_ps5.svg",
                           "PS4"    = "Figs/Scraping/icon_ps4.svg",
                           "XB1"    = "Figs/Scraping/icon_xb1.svg",
                           "XBXS"   = "Figs/Scraping/icon_xbxs.svg",
                           "Switch" = "Figs/Scraping/icon_switch.svg",
                           "Oculus" = "Figs/Scraping/icon_oculus.svg")) |> 
  nest(Platform = Platform) |> 
  select(-Date) |> 
  gt() |> 
  gt_img_rows(columns = Tier) |> 
  gt_img_multi_rows(columns = Platform, height = 15) |> 
  cols_align(columns = Platform, align = "left")
```

### 表以外の内容（複数ページ）

1位から500位まで

```r
#| label: scraping-oc-multiple
#| cache: true
oc_urls <- paste0("https://opencritic.com/browse/pc/2021?page=", 1:25)

oc_list <- list()

get_oc <- function (url) {
  temp_html <- read_html(url) |> 
    html_element(".desktop-game-display")
  temp_df <- tibble(Rank  = html_elements(temp_html, ".rank") |> 
                      html_text2(),
                    Tier  = html_elements(temp_html, "img") |> 
                      html_attr("src"),
                    Score = html_elements(temp_html, ".score") |> 
                      html_text2(),
                    Name  = html_elements(temp_html, ".game-name") |>
                      html_text2(),
                    Date  = html_elements(temp_html, ".first-release-date") |>
                      html_text2()) |> 
    mutate(Rank = str_remove(Rank, "\\."),
           Rank = as.numeric(Rank),
           Date = paste0(Date, ", 2021"),
           Date = mdy(Date))
}

for (i in seq_along(oc_urls)) {
  
  temp_df      <- get_oc(oc_urls[i])
  oc_list[[i]] <- temp_df
  
  Sys.sleep(1)
}

bind_rows(oc_list) |> 
  gt() |> 
  gt_img_rows(Tier) |> 
  cols_label("Date" = "First release date")
```
--->

## スクレイピングの注意事項

　スクレイピングの対象は自分が作成したページでなく、自分以外の個人（有志）、集団、機関などが作成したページであろう。そしてそのページ内にはその個人、集団、機関などが収集、作成、整理した資料とデータが含まれる。したがって、スクレイピングをする前に著作権を気にする必要があろう。もし、利用規約（「Terms and conditions」や「Terms of service」など）がある場合は予め読んでおこう。スクレイピングは禁止されている可能性もある。非商業的利用（教育・研究を含む）や個人利用の場合、利用が許されるケースもあるが、筆者らは弁護士でもなく、著作権に関する知識が皆無であるため、断言はできない。また、これらの法律は国によっても異なる。スクレイピングして良いかどうかが不安な場合は、そのページを作成した個人、集団、機関などに問い合わせてみるのが確実だろう。

　また、サイトによっては「ここは取得しちゃダメ」というのがある可能性もある。全てのサイトに置いてあるわけではないが、多くのサイトには`robots.txt`というファイルが置かれている[^robots]。これはウェブ・クローラー（web crawler）と呼ばれる一種のボット（Bot）向けに書かれたものであるが、スクレイピング時にも参考になろう。たとえば、OpenCriticの場合、<https://opencritic.com/robots.txt>に`robots.txt`がある。以下はその中身だ。

[^robots]: 宋の個人ホームページにも`robots.txt`は置かれているが、サイトマップ情報のみとなっている。

```{.xml filename="https://opencritic.com/robots.txt" code-line-numbers="true"}
User-agent: *
Sitemap: https://opencritic.com/sitemap.xml
Disallow: /profile
```

　`User-agent: *`は「以下の内容はすべてのクローラー（`*`）に該当する」ことを意味し、`Disallow: /profile`は「`profile`ページは取得してはいけない」ことを意味する（もし`/profile/`になっている場合は、`profile`とその以下のフォルダー全体を意味する）。ちなみに`Disallow: /`となっている場合は、サイト全体がクローリング禁止を意味する。`Disallow`とは逆の`Allow`もあり、`Allow`が`Disallow`に優先する。また、`Sitemap`は当該サイトのサイトマップ（ページ等の全体構造が記述されているファイル）のURLだ。以下の`robots.txt`はGoogleのもの（の一部）であり、ボットの種類に応じて許可範囲が異なるケースもある。

```{.xml filename="https://www.google.com/robots.txt" code-line-numbers="true"}
# AdsBot
User-agent: AdsBot-Google
Disallow: /maps/api/js/
Allow: /maps/api/js
Disallow: /maps/api/place/js/
Disallow: /maps/api/staticmap
Disallow: /maps/api/streetview

# Crawlers of certain social media sites are allowed to access page markup when google.com/imgres* links are shared. To learn more, please contact images-robots-allowlist@google.com.
User-agent: Twitterbot
Allow: /imgres
Allow: /search
Disallow: /groups
Disallow: /hosted/images/
Disallow: /m/

User-agent: facebookexternalhit
Allow: /imgres
Allow: /search
Disallow: /groups
Disallow: /hosted/images/
Disallow: /m/

Sitemap: https://www.google.com/sitemap.xml
```

　`robots.txt`に関するネット記事も多いが、筆者（宋）は[Googleのページ](https://developers.google.com/search/docs/crawling-indexing/robots/create-robots-txt?hl=ja)をおすすめする。

　そしてもう一つ重要なのは、サーバーに負担をかけないことだ。今回の例は1ページのみ、Jリーグの順位表の場合は3ページのスクレイピングであったため、サーバーへの負担はほぼないと考えられる。しかし、数百〜数千のページをスクレイピングをするケースも珍しくもない。むしろ、スクレイピングの醍醐味はそこにあろう。しかし、パソコンの動きは我々の想像をはるかに凌駕する。場合によっては1秒に数回、当該サーバーへのアクセスを試みるかも知れない。集団で特定のページをリフレッシュし続けると、サーバーへの負担が増加し、結果としてサーバーが止まることは聞いたことがあるかも知れない。何も考えずに大量のスクレイピングをすることは、そのサーバーに対し、分散型サービス妨害攻撃（DDoS攻撃）を行っていることと同じだ。したがって、ループ時に`Sys.sleep()`などを使って（時間的）間隔を空けたり、{[polite](https://dmi3kno.github.io/polite/)}パッケージを使ってサーバーへの負担を緩和したりすることも考える必要がある。また、サーバーごとに1日に許容されるトラフィックにも限度があるため、大量のデータであれば、数日に分けてスクレイピングすることも一つの方法だろう。

　スクレイピングをする時にはそのデータを作成した人や機関に敬意を払う必要があろう。