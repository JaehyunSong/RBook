# 反復処理 {#sec-iteration}

```{r}
#| include: false
source("_common.R")
```

## `*apply()`関数群と`map_*()`関数群 {#sec-iteration-intro}

まず、我らの盟友、{tidyverse}を読み込んでおきましょう。

```{r}
pacman::p_load(tidyverse)
```

それでは、`*apply()`関数群と`map_*()`関数群の動きとその仕組について調べてみましょう。まず、実習用データとして長さ5のnumericベクトル`num_vec`を用意します。

```{r}
num_vec <- c(3, 2, 5, 4, 7)
```

この`num_vec`の個々の要素に2を足す場合はどうすれば良いでしょうか。Rはベクトル単位での演算が行われるため、`num_vec + 2`だけで十分です。`+`の右側にある2は長さ1のベクトルですが、`num_vec`の長さに合わせてリサイクルされます（第[-@sec-datastructure_vector]章を参照）。

```{r}
num_vec + 2
```

賢明なRユーザーなら上のコードが正解でしょう。しかし、これからの練習のために`+`を使わずに、`for()`文を使用してみましょう（第[-@sec-programming_iteration]章を参照）。

```{r}
for (i in num_vec) {
    print(i + 2)
}
```

実はこれと同じ役割をする関数がRには内蔵されており、それが`lapply()`関数です。

```{r}
#| eval: false
lapply(オブジェクト名, 関数名, 関数の引数)
```

以下のコードからも確認出来ますが、足し算を意味する`+`も関数です。ただし、演算子を関数として使う場合は演算子を`` ` ``で囲む必要があり、`+`だと`` `+` ``と表記します。

```{r}
`+`(num_vec, 2)
```

したがって、`lapply()`関数を使用して`num_vec`の全要素に2を足す場合、以下のようなコードとなります。

```{r}
lapply(num_vec, `+`, 2)
```

これと同じ動きをする関数が{purrr}パッケージの`map()`です。{purrr}は{tidyverse}を読み込むと自動的に読み込まれます。

```{r}
map(num_vec, `+`, 2)
```

ただし、`lapply()`と`map()`の場合、戻り値はリスト型となります。もし、ベクトル型の戻り値が必要な場合は更に`unlist()`関数を使うか、`sapply()`を使います。

```{r}
# unlist()を利用し、リストを解除する
lapply(num_vec, `+`, 2) %>% unlist()

# sapply()を利用すると戻り値はベクトルとなる
sapply(num_vec, `+`, 2)
```

`map()`関数なら`unlist()`でも良いですが、`sapply()`と同じ動きをする`map_dbl()`があります。これは戻り値がdoubleのnumericベクトルになる`map()`関数です。

```{r}
# map_dbl()を利用するとnumeric (double)のベクトルが返される
map_dbl(num_vec, `+`, 2)
```

もし、2を足すだけでなく、更に3で割るためにはどうすれば良いでしょうか。まず考えられるのは更に`sapply()`や`map_dbl`を使うことです。

```{r}
map_dbl(num_vec, `+`, 2) %>% 
    map_dbl(`/`, 3)
```

もう一つの方法は`sapply()`や`map_dbl()`の第二引数に直接関数を指定する方法です。

```{r}
sapply(num_vec, function(x){(x + 2) / 3})

map_dbl(num_vec, function(x){(x + 2) / 3})
```

上のコードだと、`num_vec`の要素が第二引数で指定した関数の引数（`x`）として用いられます。関数が長くなる場合は、`sapply()`や`map()`の外側に関数を予め指定して置くことも可能です。

```{r}
add_two_divide_three <- function(x){
    (x + 2) / 3 
}
sapply(num_vec, add_two_divide_three)

map_dbl(num_vec, add_two_divide_three)
```

ここまでの例だと`sapply()`と`map_dbl()`はほぼ同じ関数です。なぜわざわざ{purrr}パッケージを読み込んでまで`map_dbl()`関数を使う必要があるでしょうか。それは`map_dbl()`の内部にはラムダ関数（lambda function; ラムダ関数）、あるいは無名関数（anonymous function）と呼ばれるものが使用可能だからです。ラムダ関数は第[-@sec-handling2-summarise]章でも説明しましたが、もう一回解説します。ラムダ関数は使い捨ての関数で、`map_dbl()`内部での処理が終わるとメモリ上から削除される関数です。使い捨てですので、関数の名前（オブジェクト名）も与えられておりません。

このラムダ関数の作り方ですが、`~`で始まり、引数の部分には`.x`が入ります[^iter-lambda]。したがって、`~(.x + 2) / 3`は`function(x){(x + 2) / 3}`の簡略したものとなります。ただし、後者だと無名関数の引数に該当する`x`を`y`や`j`などに書き換えても問題ありませんが、ラムダ関数では必ず`.x`と表記する必要があります。

[^iter-lambda]: 引数が2つなら`.x`と`.y`を使用します。もし3つ以上なら`..1`、`..2`、`..3`、...と表記します。

```{r}
map_dbl(num_vec, ~(.x + 2) / 3)
```

かなり短めなコードで「`num_vec`の全要素に2を足して3で割る」処理ができました。一方、`sapply()`や`lapply()`のような`*apply()`関数群だとラムダ関数を使うことはできません。現在のメモリ上にある関数のみしか使うことができません。

```{r}
#| error: true
sapply(num_vec, ~(.x + 2) / 3)
```

これまでの例は**正しい**コードではありますが、**良い**コードとは言えないでしょう。なぜなら`num_vec + 2`という最適解が存在するからです。`*apply()`と`map_*()`はより複雑な処理に特化しています。たとえば、リスト型データの処理です。以下の例を考えてみましょう。

```{r}
num_list <- list(List1 = c(1, 3, 5, 7, 9),
                 List2 = c(2, 4, 6, 8, 10, 13, 3),
                 List3 = c(3, 2, NA, 5, 8, 9, 1))
```

`num_list`は3つのnumeric型ベクトルで構成されたリスト型オブジェクトです。それぞれのベクトルの平均値を求めてみましょう。これを普通に`mean()`関数のみで済まそうとすると、リストからベクトルを一つずつ抽出し、3回の`mean()`関数を使用する必要があります、

```{r}
mean(num_list[["List1"]], na.rm = TRUE)
mean(num_list[["List2"]], na.rm = TRUE)
mean(num_list[["List3"]], na.rm = TRUE)
```

もしリストの長さが大きくなると、以下のように`for()`文の方が効率的でしょう。

```{r}
for (i in names(num_list)) {
    print(mean(num_list[[i]], na.rm = TRUE))
}
```

計算結果をベクトルとして出力/保存する場合は予めベクトルを用意しておく必要があります。

```{r}
# num_listの長さと同じ長さの空ベクトルを生成
Return_vec <- rep(NA, length(num_list))

for (i in 1:length(num_list)) {
    Return_vec[i] <- mean(num_list[[i]], na.rm = TRUE)
}

Return_vec
```

以上の例は`sapply()`、または`map_dbl`関数を使うとより短くすることができます。

```{r}
sapply(num_list, mean, na.rm = TRUE)

map_dbl(num_list, mean, na.rm = TRUE)
```

`*apply()`も`map_*()`も、それぞれの要素に対して同じ処理を行うことを得意とする関数です。他の応用としては、各ベクトルからn番目の要素を抽出することもできます。ベクトルからn番目の要素を抽出するには`ベクトル名[n]`と入力しますが、実はこの`[`も関数です。関数として使う場合は`+`と同様、`` `[` ``と表記します。`num_list`内の3つのベクトルから3番目の要素を抽出してみましょう[^map-extract]。

[^map-extract]: 実は今回の例のように要素を抽出する場合、`` `[` ``すら要りません。`map_dbl(num_list, 3)`だけで十分です。

```{r}
map_dbl(num_list, `[`, 3)
```

ここまで来たら`map_*()`関数群の仕組みについてイメージが出来たかと思います。`map_*()`関数群の動きは @fig-map_inside のように表すことができます。第一引数はデータであり、そのデータの各要素に対して第二引数で指定された関数を適用します。この関数に必要な（データを除く）引数は第三引数以降に指定します。この関数部（第二引数）はRやパッケージなどで予め提供されている関数でも、内部に直接無名関数を作成することもできます。この無名関数は`function(x){}`のような従来の書き方も可能ですが、`map_*()`関数群の場合`~`で始まるラムダ関数を使うことも可能です。

![map関数群のイメージ](Figs/Iteration/map_inside.png){#fig-map_inside width="85%"}

`map()`の場合、返り値はリストとなり、`map_dbl()`の返り値はnumeric (double)型のベクトルとなります。他にも`map_*()`関数群には`map_int()`、`map_lgl()`、`map_chr()`、`map_df()`などがあり、それぞれ返り値のデータ型/データ構造を表しています。例えば、返り値がcharacter型のベクトルであれば、`map_chr()`を使います。`c(1, 2, 3, 4, 5)`のベクトルの各要素の前に`"ID: "`を付ける例だと以下のように書きます。

```{r}
# 以下のコードでもOK
# map_chr(c(1, 2, 3, 4, 5), ~paste0("ID: ", .x))
c(1, 2, 3, 4, 5) %>%
  map_chr(~paste0("ID: ", .x))
```

---

## 引数が2つ以上の場合 {#sec-iteration-map2}

{purrr}を使った本格的な例を紹介する前に、引数が2つ以上の場合を考えたいと思います。まずは引数が2つの場合です。この場合、`map2_*()`関数群を使用します。例えば、`num_vec`に長さ5のnumeric型ベクトル`num_vec2`をかける例を考えてみましょう。

この場合、データとして`num_vec`と`num_vec2`が必要となり、ラムダ関数にも2つの引数が必要です。まず、`num_vec2`を用意します。

```{r}
num_vec2 <- c(1, 0, 1, 0, 1)
```

続いて`map2_dbl()`関数を使用し`num_vec`と`num_vec2`の掛け算を行います。`map2_*()`の使い方は`map_*()`とほぼ同様です。

```{r}
#| eval: false
map2_dbl(データ1, データ2, 関数 or ラムダ関数, 追加の引数)
```

`map_*()`との違いとしては、(1) データが2つである、(2) 関数、またはラムダ関数に2つの引数が必要である点です。この2点目の引数ですが、データ2は`.y`と表記します。したがって、データ1とデータ2の掛け算を意味するラムダ関数は`~.x * .y`です。

```{r}
map2_dbl(num_vec, num_vec2, ~.x * .y)
```

`num_vec`と`num_vec2`が必ずしも同じデータ構造、データ型、同じ長さである必要がありません。数字の前に`"ID:"`を付ける先ほどの例を`map2_chr()`で書いてみましょう。

```{r}
map2_chr(num_vec, "ID:", ~paste0(.y, .x))
```

それでは3つ以上の引数について考えてみましょう。たとえば、`num_vec`の前に`"ID"`を付けるとします。そして`"ID"`と`num_vec`の間に`":"`を入れたい場合はどうすればい良いでしょう。むろん、賢い解決方法は単純に`paste()`関数を使うだけです。

```{r}
paste("ID", num_vec, sep = ":")
```

この場合、引数は3つです[^iter-paste]。この場合の書き方はどうなるでしょうか。`map2_*()`はデータを2つまでしか指定できません。3つ目以降は関数/ラムダ関数の後ろに書くこととなります。ただし、関数/ラムダ関数の後ろに指定される引数は長さ1のベクトルでなければなりません。また、ラムダ関数内の引数は`.x`と`.y`でなく、`..1`、`..2`、`..3`、...となります。

[^iter-paste]: `paste()`関数は`sep = `以外はすべて結合の対象となります。引数が4つ以上になることもあります。たとえば、`paste("We", "love", "cats!", sep = " ")`です。

今回の例だと`map2_chr()`内に`num_vec`がデータ1、`"ID"`がデータ2です。そして、期待される結果は、「"ID" + `sep`の実引数 + `num_vec`の値」となります。したがって、ラムダ関数は`paste(..2, ..1, sep = ..3)`となります。

```{r}
map2_chr(num_vec, "ID", ~paste(..2, ..1, sep = ..3), "-")
```

データ2である`"ID"`は長さ1のcharacter型ベクトルであるため、以下のように`map_chr()`を使うことも可能です。

```{r}
# データ2も長さ1なのでmap_chr()もOK
map_chr(num_vec, ~paste(..2, ..1, sep = ..3), "ID", "-")
```

それではデータを3つ以上使うにはどうすれば良いでしょうか。そこで登場するのが`pmap_*()`関数です。以下の3つのベクトルを利用し、「名前:数学成績」を出力してみましょう。ただし、不正行為がある場合（`cheat_vec`の値が1）は成績が0点になるようにしましょう。

```{r}
name_vec  <- c("Hadley", "Song", "Yanai")
math_vec  <- c(70, 55, 80)
cheat_vec <- c(0, 1, 0)
```

賢い解決法は普通に`paste0()`関数を使う方法です。

```{r}
paste0(name_vec, ":", math_vec * (1 - cheat_vec))
```

今回はあえて`pmap_*()`関数を使ってみましょう。`pmap_*()`の場合、第一引数であるデータはリスト型で渡す必要があります。したがって、3つのベクトルを`list()`関数を用いてリスト化します。第二引数には既存の関数やラムダ関数を入力し、各引数は`..1`、`..2`、...といった形で表記します。

```{r}
pmap_chr(list(name_vec, math_vec, cheat_vec), 
         ~paste0(..1, ":", ..2 * (1 - ..3)))
```

第一引数はデータであるため、まずリストを作成し、パイプ演算子（`%>%`）で`pmap_*()`に渡すことも可能です。

```{r}
list(name_vec, math_vec, cheat_vec) %>%
  pmap_chr(~paste0(..1, ":", ..2 * (1 - ..3)))
```

---

## データフレームと{purrr} {#sec-iteration-df}

`map_*()`関数のデータとしてデータフレームを渡すことも可能です。ここでは5行3列のデータフレームを作成してみましょう。

```{r}
Dummy_df <- data.frame(X = seq(1,  5,  by = 1),
                       Y = seq(10, 50, by = 10),
                       Z = seq(2,  10, by = 2))
```

各行の`X`、`Y`、`Z`の合計を計算するには{dplyr}の`rowwise()`と`mutate()`を組み合わせることで計算出来ることを第[-@sec-handling3-rowwise]章で紹介しました。

```{r}
Dummy_df %>%
  rowwise() %>%
  mutate(Sum = sum(X, Y, Z)) %>%
  # rowwise()は1行を1グループとする関数であるため、最後にグループ化を解除
  ungroup()
```

それでは各列の平均値を計算するにはどうすれば良いでしょうか。ここではR内蔵関数である`colMeans()`を使わないことにしましょう。

```{r}
colMeans(Dummy_df)
```

まず、`mean(Dummy_df$X)`を3回実行する方法や、{dplyr}の`summarise()`関数を使う方法があります。

```{r}
Dummy_df %>%
  summarise(X = mean(X), 
            Y = mean(Y),
            Z = mean(Z))
```

実はこの操作、`map_dbl()`関数を使えば、より簡単です。

```{r}
map_dbl(Dummy_df, mean)
```

`map_dbl()`はnumeric (double) 型ベクトルを返しますが、データフレーム（具体的にはtibble）に返すなら`map_df()`を使います。

```{r}
map_df(Dummy_df, mean)
```

なぜこれが出来るでしょうか。これを理解するためにはデータフレームとtibbleが本質的にはリスト型と同じであることを理解する必要があります。たとえば、以下のようなリストについて考えてみましょう。

```{r}
Dummy_list <- list(X = seq(1,  5,  by = 1),
                   Y = seq(10, 50, by = 10),
                   Z = seq(2,  10, by = 2))

Dummy_list
```

この`Dummy_list`を`as.data.frame()`関数を使用して強制的にデータフレームに変換してみましょう。

```{r}
as.data.frame(Dummy_list)
```

`Dummy_df`と同じものが出てきました。逆に`Dummy_df`を、`as.list()`を使用してリストに変換すると`Dummy_list`と同じものが返されます。

```{r}
as.list(Dummy_df)
```

ここまで理解できれば、`map_*()`関数のデータがデータフレームの場合、内部ではリストとして扱われることが分かるでしょう。実際、`Dummy_list`をデータとして入れても`Dummy_df`を入れた結果と同じものが得られます。

```{r}
map_df(Dummy_list, mean)
```

### tibbleの話 {#sec-iteration-tibble}

ここまで「データフレーム」と「tibble」を区別せずに説明してきましたが、これからの話しではこの2つを区別する必要があります。tibbleは{tidyverse}のコアパッケージの一つである{tibble}が提供するデータ構造であり、データフレームの上位互換です。tibbleもデータフレーム同様、本質的にはリストですが、リストの構造をより的確に表すことが出来ます。

データフレームをリストとして考える場合、リストの各要素は**必ずベクトル**である必要があります。たとえば、`Dummy_list`には3つの要素があり、それぞれ長さ5のベクトルです。一方、リストの中にはリストを入れることも出来ます。たとえば、以下のような`Dummy_list2`について考えてみましょう。

```{r}
Dummy_list2 <- list(ID   = 1:3, 
                    Data = list(Dummy_df, Dummy_df, Dummy_df))

Dummy_list2
```

`Dummy_list2`には2つの要素があり、最初の要素は長さ3のベクトル、2つ目の要素は長さ3のリストです。2つ目の要素がベクトルでないため、`Dummy_list2`をデータフレームに変換することはできません。

```{r}
#| error: true
Dummy_df2 <- as.data.frame(Dummy_list2)
```

一方、`as_tibble()`を使用してtibble型に変換することは可能です。

```{r}
Dummy_tibble <- as_tibble(Dummy_list2)

Dummy_tibble
```

2列目の各セルには5行3列のデータフレーム（`df`）が格納されていることが分かります。たとえば、`Dummy_tibble`の`Data`列を抽出してみましょう。

```{r}
Dummy_tibble$Data
```

長さ3のリスト出力されます。続いて、`Dummy_tibble$Data`の2番目のセルを抽出してみましょう。

```{r}
Dummy_tibble$Data[2]
```

データフレームが出力されました。簡単にまとめるとtibbleはデータフレームの中にデータフレームを入れることが出来るデータ構造です。むろん、これまでの通り、データフレームのように使うことも可能です[^iter-tibble]。これがtibbleの強みでもあり、{purrr}との相性も非常に高いです。たとえば、`Data`列を`map()`関数のデータとして渡せば、複数のデータセットに対して同じモデルの推定が出来るようになります。以下ではその例を紹介します。

[^iter-tibble]: ただしtibbleは`tibble名[3, 2:4] <- c("A", "B", "C")`のような、列をまたがった値の代入は出来ません。これによってtibbleに対応しない関数もまだあります。この場合、`as.data.frame()`を使って一旦データフレームに変換する必要があります。

---

## モデルの反復推定 {#sec-iteration-model1}

ここからは`map_*()`関数群を使って複数のモデルを素早く推定する方法について解説します。サンプルデータは第[-@sec-visualization2]章で使いました各国の政治経済データ（`Countries.csv`）を使用します。csv形式データ読み込みの際、これまではR内蔵関数である`read.csv()`と{readr}[^iter-visual]の`read_csv()`を区別せずに使用してきました。しかし、ここからは`read_csv()`を使用します。2つの使い方はほぼ同じですが、`read_csv()`は各列のデータ型を自動的に指定してくれるだけではなく、tibble構造として読み込みます。`read.csv()`を使用する場合、`as_tibble(データフレーム名)`でデータフレームをtibbleに変換してください。

[^iter-visual]: {tidyverse}のコア・パッケージであるため、別途読み込む必要はございません。

```{r}
#| message: false
Country_df <- read_csv("Data/Countries.csv")
```

### サンプルの分割とモデル推定（`split()`利用） {#sec-iteration-split1}

まずは、サンプルを分割し、それぞれの下位サンプルを使った分析を繰り返す方法について解説します。サンプルを分割する方法は2つありますが、最初はR内蔵関数である`split()`関数を使った方法について紹介します。

```{r}
#| eval: false
# split()の使い方
新しいオブジェクト名 <- split(データ名, 分割の基準となるベクトル)
```

たとえば、`Country_df`を大陸ごとにサンプルを分けるとします。大陸を表すベクトルは`Country_df$Continent`です。したがって、以下のように入力します。

```{r}
Split_Data <- split(Country_df, Country_df$Continent)
```

サンプルが分割された`Split_Data`のデータ構造はリスト型です。

```{r}
class(Split_Data)
```

中身を見るとリストの各要素としてtibble（データフレーム）が格納されています。リストの中にはあらゆるデータ型、データ構造を入れることができることが分かります。

```{r}
Split_Data
```

それでは分割された各サンプルに対して`Polity_Score`と`FH_Total`の相関係数を計算してみましょう。その前に相関分析の方法について調べてみましょう。Rには相関分析の関数が2つ用意されています。単純に相関係数のみを計算するなら`cor()`、係数の不確実性（標準誤差、信頼区間など）まで計算し、検定を行うなら`cor.test()`を使用します。ここではより汎用性の高い`cor.test()`の使い方について紹介します。

```{r}
#| eval: false
# 相関分析: 方法1
cor.test(~ 変数名1 + 変数名2, data = データ名)

# 相関分析: 方法2
cor.test(データ名$変数名1, データ名$変数名2)
```

それでは全サンプルに対して相関分析をしてみましょう。

```{r}
Cor_fit1 <- cor.test(~ Polity_Score + FH_Total, data = Country_df)
Cor_fit1
```

ここから相関係数（`r Cor_fit1$estimate`）のみを抽出するにはどうすれば良いでしょうか。それを確認するためには`Cor_fit1`というオブジェクトの構造を調べる必要があります。ここではR内蔵関数である`str()`を使って確認してみましょう。

```{r}
str(Cor_fit1)
```

相関係数は`$estimate`で抽出できそうですね。実際に`Cor_fit1`から相関係数のみ抽出してみましょう。

```{r}
Cor_fit1$estimate
```

それでは`map()`関数を利用して分割された各サンプルを対象に`Polity_Score`と`FH_Total`の相関係数を計算してみましょう。`map()`のデータは`Split_Data`とし、関数はラムダ関数を書いてみましょう。`cor.test()`内`data`引数の実引数は`.x`、または`..1`となります。最後に`map_dbl("estimate")`を利用し、相関係数を抽出、numeric (double) 型ベクトルとして出力します。

```{r}
Cor_fit2 <- Split_Data %>%
  map(~cor.test(~ Polity_Score + FH_Total, data = .x))
```

```{r}
# Cor_fit2から相関係数のみ出力
map_dbl(Cor_fit2, "estimate")
```

もし、小数点3位までの*p*値が欲しい場合は以下のように入力します。

```{r}
# Cor_fit2から相関係数のp値を抽出し、小数点3位に丸める
map_dbl(Cor_fit2, "p.value") %>% round(3)
```

### サンプルの分割とモデル推定（`nest()`利用） {#sec-iteration-split2}

それではデータフレーム内にデータフレームが格納可能なtibble構造の長所を活かした方法について解説します。ある変数に応じてデータをグループ化する方法については第[-@sec-handling2-group]章で解説しました。{tidyr}パッケージにはグループごとにデータを分割し、分割されたデータを各セルに埋め込む`nest()`関数を提供しています。具体的な動きを見るために、とりあえず、`Country_df`を大陸（`Continent`）ごとに分割し、それぞれがデータが一つのセルに埋め込まれた新しいデータセット、`Nested_Data`を作ってみましょう。

```{r}
Nested_Data <- Country_df %>% 
  group_by(Continent) %>%
  nest()
```

ちなみに`group_by()`と`nest()`は`group_nest()`を使って以下のようにまとめることも可能です。

```{r}
#| eval: false
Nested_Data <- Country_df %>% 
  group_nest(Continent)
```

```{r}
Nested_Data
```


2列目の`data`変数の各セルにtibbleが埋め込まれたことがわかります。`Nested_Data`の`data`列から5つ目の要素（オセアニアのデータ）を確認してみましょう。

```{r}
# Nested_Data$data[5]の場合、長さ1のリストが出力される
# tibble構造で出力する場合は[5]でなく、[[5]]で抽出
Nested_Data$data[[5]]
```

`Continent`がOceaniaの行の`data`列にオセアニアのみのデータが格納されていることが分かります。このようなデータ構造を入れ子型データ（nested data）と呼びます。この入れ子型データは`unnest()`関数を使って解除することも可能です。`unnest()`関数を使う際はどの列を解除するかを指定する必要があり、今回は`data`列となります。

```{r}
# 入れ子型データの解除
Nested_Data %>%
  unnest(data)
```

{dplyr}の`group_by()`関数と{tidyr}の`nest()`、`unnest()`関数を組み合わせることでデータを入れ子型データへ変換したり、解除することができます。以上の流れを図式化したものが以下の @fig-nested_data です。

![入れ子型データの生成過程](Figs/Iteration/nested_data.png){#fig-nested_data width="85%"}

それではこの入れ子型データを使用して大陸ごとの`Polity_Score`と`FH_Total`の相関係数を計算してみましょう。`data`列をデータとした相関係数のラムダ関数はどう書けば良いでしょうか。これまでの内容が理解できましたら、答えは難しくないでしょう。`cor.test()`関数の`data`引数の実引数として`.x`を指定するだけです。この結果を`Cor_test`という列として追加し、`Nested_Data2`という名のオブジェクトとして保存します。

```{r}
Nested_Data2 <- Nested_Data %>%
  mutate(Cor_test = map(data, ~cor.test(~ Polity_Score + FH_Total, data = .x)))

Nested_Data2
```

`Cor_test`という列が追加され、htestというクラス（S3クラス）オブジェクトが格納されていることが分かります。ちゃんと相関分析のオブジェクトが格納されているか、確認してみましょう。

```{r}
Nested_Data2$Cor_test
```

5つの相関分析結果が格納されています。続いて、ここから相関係数のみを抽出してみましょう。相関分析オブジェクトから相関係数を抽出するには`オブジェクト名$estimate`だけで十分です。`mpa_*()`関数を使うなら第2引数として関数やラムダ関数を指定せず、`"estimate"`のみ入力するだけです。

```{r}
Nested_Data2 <- Nested_Data2 %>%
  mutate(Cor_coef = map(Cor_test, "estimate"))

Nested_Data2
```

`Cor_coef`列が追加され、それぞれのセルにnumeric型の値が格納されていることが分かります。`map()`関数はリスト型でデータを返すため、このように出力されます。この`Cor_coef`列の入れ子構造を解除してみましょう。

```{r}
Nested_Data2 <- Nested_Data2 %>%
  unnest(cols = Cor_coef)

Nested_Data2
```

`Cor_coef`の各列に格納された値が出力されました。以上の作業を一つのコードとしてまとめることも出来ます。また、相関係数の抽出の際に`map()`でなく、`map_dbl()`を使えば、numeric型ベクトルが返されるので`unnest()`も不要となります。

```{r}
Country_df %>%
  group_by(Continent) %>%
  nest() %>%
  mutate(Cor_test = map(data, ~cor.test(~ Polity_Score + FH_Total, data = .x)),
         Cor_coef = map_dbl(Cor_test, "estimate"))
```

たった5行のコードで大陸ごとの相関分析が出来ました。これを`map_*()`を使わずに処理するなら以下のようなコードとなります[^iter-inconvenience]。

[^iter-inconvenience]: `cor.test()$estimate`で直接相関係数を抽出することも可能ですが、コードの可読性が著しく低下します。またオブジェクトの再利用（例えば、「今度は*p*値を抽出しよう！」）が出来なくなります。

```{r}
Cor_Test1 <- cor.test(~ Polity_Score + FH_Total, 
                      data = subset(Country_df, Country_df$Continent == "Asia"))
Cor_Test2 <- cor.test(~ Polity_Score + FH_Total, 
                      data = subset(Country_df, Country_df$Continent == "Europe"))
Cor_Test3 <- cor.test(~ Polity_Score + FH_Total, 
                      data = subset(Country_df, Country_df$Continent == "Africa"))
Cor_Test4 <- cor.test(~ Polity_Score + FH_Total, 
                      data = subset(Country_df, Country_df$Continent == "America"))
Cor_Test5 <- cor.test(~ Polity_Score + FH_Total, 
                      data = subset(Country_df, Country_df$Continent == "Oceania"))

Cor_Result <- c("Asia" = Cor_Test1$estimate, "Europe" = Cor_Test2$estimate,
                "Africa" = Cor_Test3$estimate, "America" = Cor_Test4$estimate,
                "Oceania" = Cor_Test5$estimate)

print(Cor_Result)
```

それでは応用例として回帰分析をしてみましょう。今回も`Nested_Data`を使用し、`PPP_per_capita`を応答変数に、`FH_Total`と`Population`を説明変数とした重回帰分析を行い、政治的自由度（`FH_Total`）の係数や標準誤差などを抽出してみましょう。まずは、ステップごとにコードを分けて説明し、最後には一つのコードとしてまとめたものをお見せします。

```{r}
Nested_Data %>%
  mutate(Model = map(data, 
                     ~lm(PPP_per_capita ~ FH_Total + Population, 
                         data = .x)))
```

`Model`列からから推定結果の要約を抽出するにはどうすれば良いでしょうか。1つ目の方法は`summary(lmオブジェクト名)$coefficients`で抽出する方法です。

```{r}
lm_fit1 <- lm(PPP_per_capita ~ FH_Total + Population, data = Country_df)

summary(lm_fit1)$coefficients
```

もっと簡単な方法は{broom}の`tidy()`関数を使う方法です。`tidy()`関数は推定値に関する様々な情報をデータフレームとして返す便利な関数です。デフォルトだと95%信頼区間は出力されませんが、`conf.int = TRUE`を指定すると信頼区間も出力されます。`tidy()`関数を提供する{broom}パッケージは{tidyverse}の一部ですが、{tidyverse}読み込みの際に一緒に読み込まれるものではないため、予め{broom}パッケージを読み込んでおくか、`broom::tidy()`で使うことができます。

```{r}
broom::tidy(lm_fit1, conf.int = TRUE)
```

それでは`Model`列にあるlmオブジェクトから推定値の情報を抽出し、`Model2`という列に格納してみましょう。今回はラムダ関数を使う必要がありません。なぜなら、`tidy()`の第一引数がデータだからです（`?tidy.lm`参照）。他にも必要な引数（`conf.int = TRUE`など）があれば、`map()`の第3引数以降で指定します。

```{r}
Nested_Data %>%
  mutate(Model  = map(data, 
                      ~lm(PPP_per_capita ~ FH_Total + Population, 
                          data = .x)),
         Model2 = map(Model, broom::tidy, conf.int = TRUE))
```

`Model2`列の各セルに7列のtibbleが格納されているようです。ここからは`data`と`Model`列は不要ですので`select()`関数を使って除去し、`Model2`の入れ子構造を解除してみます。

```{r}
Nested_Data %>%
  mutate(Model  = map(data, 
                      ~lm(PPP_per_capita ~ FH_Total + Population, 
                          data = .x)),
         Model2 = map(Model, broom::tidy, conf.int = TRUE)) %>%
  select(-c(data, Model)) %>%
  unnest(cols = Model2)
```

各大陸ごとの回帰分析の推定結果が一つのデータフレームとして展開されました。ここでは`FH_Total`の推定値のみがほしいので、`filter()`関数を使用し、`term`の値が`"FH_Total"`の行のみを残します。また、`term`列も必要なくなるので除外しましょう。

```{r}
Nested_Data %>%
  mutate(Model  = map(data, 
                      ~lm(PPP_per_capita ~ FH_Total + Population, 
                          data = .x)),
         Model2 = map(Model, broom::tidy, conf.int = TRUE)) %>%
  select(-c(data, Model)) %>%
  unnest(cols = Model2) %>%
  filter(term == "FH_Total") %>%
  select(-term)
```

これで終わりです。政治的自由度と所得水準の関係はアジアとアフリカでは連関の程度が小さく、統計的有意ではありません。オセアニアの場合、連関の程度は大きいと考えられますが、サンプルサイズが小さいため統計的有意な結果は得られませんでした。一方、ヨーロッパとアメリカでは連関の程度も大きく、統計的有意な連関が確認されました。

以上のコードをよりコンパクトにまとめると以下のようなコードとなります。

```{r}
#| eval: false
# {purrr}使用
Nested_Data %>%
  mutate(Model = map(data, ~lm(PPP_per_capita ~ FH_Total + Population, data = .x)),
         Model = map(Model, broom::tidy, conf.int = TRUE)) %>%
  unnest(cols = Model) %>%
  filter(term == "FH_Total") %>%
  select(-c(Data, term))
```

{purrr}パッケージを使用しない例も紹介します。むろん、以下のコードは可能な限り面倒な書き方をしています。`for()`文や`split()`と`*apply()`関数を組み合わせると以下の例よりも簡潔なコードは作成できます。R上級者になるためには{tidyverse}的な書き方だけでなく、ネイティブRの書き方にも慣れる必要があります。ぜひ挑戦してみましょう。

```{r}
#| eval: false
# {purrr}を使用しない場合
# FH_Totalの係数と標準誤差のみを抽出する例
lm_fit1 <- lm(PPP_per_capita ~ FH_Total + Population, 
              data = subset(Country_df, Country_df$Continent == "Asia"))
lm_fit2 <- lm(PPP_per_capita ~ FH_Total + Population, 
              data = subset(Country_df, Country_df$Continent == "Europe"))
lm_fit3 <- lm(PPP_per_capita ~ FH_Total + Population, 
              data = subset(Country_df, Country_df$Continent == "Africa"))
lm_fit4 <- lm(PPP_per_capita ~ FH_Total + Population, 
              data = subset(Country_df, Country_df$Continent == "America"))
lm_fit5 <- lm(PPP_per_capita ~ FH_Total + Population, 
              data = subset(Country_df, Country_df$Continent == "Oceania"))

lm_df <- data.frame(Continent = c("Asia", "Europe", "Africa", "America", "Oceania"),
                    estimate  = c(summary(lm_fit1)$coefficients[2, 1],
                                  summary(lm_fit2)$coefficients[2, 1],
                                  summary(lm_fit3)$coefficients[2, 1],
                                  summary(lm_fit4)$coefficients[2, 1],
                                  summary(lm_fit5)$coefficients[2, 1]),
                    se        = c(summary(lm_fit1)$coefficients[2, 2],
                                  summary(lm_fit2)$coefficients[2, 2],
                                  summary(lm_fit3)$coefficients[2, 2],
                                  summary(lm_fit4)$coefficients[2, 2],
                                  summary(lm_fit5)$coefficients[2, 2]))
```

### データの範囲を指定したモデル推定 {#sec-iteration-range}

これまでの例は名目変数でグループ化を行いましたが、連続変数を使うことも可能です。たとえば、人口1千万未満の国、1千万以上5千万未満、5千万以上1億未満、1億以上でサンプルを分割することです。まず、`Country_df`から`PPP_per_capita`、`FH_Total`、`Population`列のみを抽出し、`Country_df2`に格納します。

```{r}
Country_df2 <- Country_df %>%
  select(PPP_per_capita, FH_Total, Population)
```

続いて、`case_when()`関数を使用し、`Population`の値を基準にケースがどの範囲内に属するかを表す変数`Group`を作成します。

```{r}
Country_df2 <- Country_df2 %>%
  mutate(Group = case_when(Population <  10000000  ~ "1千万未満",
                           Population <  50000000  ~ "1千万以上5千万未満",
                           Population <  100000000 ~ "5千万以上1億未満",
                           Population >= 100000000 ~ "1億以上"))
```

中身を確認してみましょう。

```{r}
Country_df2
```

あとはこれまでの例と同じ手順となります。まず、データを`Group`変数でグループ化し、入れ子構造に変換します。

```{r}
Country_df2 <- Country_df2 %>%
  group_by(Group) %>%
  nest()
```

変換後のデータを確認してみます。

```{r}
Country_df2
```

続いて、`data`列をデータとし、`map()`関数でモデルの推定をしてみましょう。目的変数は所得水準、説明変数は政治的自由度とします。推定後、{broom}の`tidy()`変数で推定値の情報のみを抽出し、入れ子構造を解除します。最後に`term`がFH_Totalの行のみを残します。

```{r}
Country_df2 <- Country_df2 %>%
  mutate(Model = map(data, ~lm(PPP_per_capita ~ FH_Total, data = .x)),
         Est   = map(Model, broom::tidy, conf.int = TRUE)) %>%
  unnest(Est) %>%
  filter(term == "FH_Total") %>%
  select(!term)

Country_df2
```

せっかくなので推定結果を可視化してみましょう。横軸は人口規模（`Group`）とし、縦軸は推定値とします。係数の点推定値と95%信頼区間を同時に出力するために、`geom_pointrange()`幾何オブジェクトを使用します。

```{r}
#| label: fig-iteration1
#| fig-cap: "政治的自由度が所得に与える影響 (人口規模別)"
Country_df2 %>%
  mutate(
    # 横軸の表示順番を指定するために、Group変数をfactor化する
    Group = factor(Group, levels = c("1千万未満", "1千万以上5千万未満",
                                     "5千万以上1億未満", "1億以上")),
    # 統計的有意か否かを表すSig変数の作成
    Sig   = if_else(conf.low * conf.high > 0, "統計的有意", "統計的非有意")
    ) %>%
  ggplot() + 
  geom_hline(yintercept = 0, color = "red") +
  geom_pointrange(aes(x = Group, y = estimate, 
                      ymin = conf.low, ymax = conf.high, color = Sig), 
                  size = 0.75) +
  labs(x = "人口", y = "政治的自由度が所得に与える影響", color = "") +
  scale_color_manual(values = c("統計的有意" = "black",
                                "統計的非有意" = "gray70")) +
  theme_minimal(base_size   = 12) +
  theme(legend.position = "bottom")
```

政治的自由度が高くなるほど所得水準も高くなる傾向が確認されますが、人口が1億人以上の国においてはそのような傾向が見られないことが分かります。

上記の例は、ある行は一つのグループに属するケースです。しかし、ある行が複数のグループに属するケースもあるでしょう。たとえば、ある変数の値が一定値以上である行のみでグループ化する場合です。`FH_Score`が一定値以上のSub-sampleに対して回帰分析を行う場合、`FH_Score`が最大値（100）である国はすべてのSub-sampleに属することになります。この場合は`group_by()`でグループ化することが難しいかも知れません。しかし、{dplyr}の`filter()`を使えば簡単に処理することができます。

ここでは人口を説明変数に、所得水準を応答変数とした単回帰分析を行います。データは政治的自由度が一定値以上の国家のみに限定します。`FH_Score`が0以上の国（すべてのケース）、10以上の国、20以上の国、...などにサンプルを分割してみましょう。まずは`FH_Score`の最小値のみを格納した`Rnage_df`を作成します。

```{r}
Range_df <- tibble(Min_FH = seq(0, 80, by = 10))

Range_df
```

9行1列のtibbleが出来ました。続いて、`Subset`という列を生成し、ここにデータを入れてみましょう。ある変数の値を基準にサンプルを絞るには{dplyr}の`filter()`関数を使用します。たとえば、`Counter_df`の`FH_Total`が50以上のケースに絞るには`filter(Country_df, FH_Total >= 50)`となります。パイプ演算子を使った場合は`Country_df %>% filter(FH_Total >= 50)`になりますが、ラムダ関数とパイプ演算子の相性はあまり良くありませんので、ここではパイプ演算子なしとします。重要なのは`Min_FH`の値をデータとした`map()`関数を実行し、実行内容を「`Country_df`から`FH_Total`が`Min_FH`以上のケースのみを抽出せよ」にすることです。

```{r}
Range_df <- Range_df %>%
  mutate(Subset = map(Min_FH, ~filter(Country_df, FH_Total >= .x)))

Range_df
```

入れ子構造になっているのは確認できましたが、ちゃんとフィルタリングされているかどうかも確認してみましょう。たとえば、`Range_df$Subset[[9]]`だと`FH_Total`が80以上の国に絞られていると考えられます。18列の表ですから`Country`と`FH_Total`列のみを確認してみましょう。

```{r}
Range_df$Subset[[9]] %>%
  select(Country, FH_Total)
```

問題なくフィルタリングされているようですね。ここまで出来ればあとはこれまでの内容の復習でしょう。

```{r}
Range_df <- Range_df %>%
  mutate(Model  = map(Subset, ~lm(PPP_per_capita ~ Population, data = .x)),
         Model  = map(Model,  broom::tidy, conf.int = TRUE)) %>%
  unnest(cols = Model) %>%
  filter(term == "Population") %>%
  select(-c(Subset, term))
```

今回も可視化してみましょう。

```{r}
#| label: fig-iteration2
#| fig-cap: "Populationの係数の推定値"
Range_df %>%
  ggplot() +
  geom_hline(yintercept = 0, color = "red") +
  geom_pointrange(aes(x = Min_FH, y = estimate, 
                      ymin = conf.low, ymax = conf.high)) +
  labs(x = "データ内FH_Scoreの最小値", y = "Populationの係数") +
  scale_x_continuous(breaks = seq(0, 80, by = 10),
                     labels = seq(0, 80, by = 10)) +
  theme_minimal(base_size = 12)
```

以上の例は実際の分析では多く使われる方法ではないかも知れません。しかし、ノンパラメトリック回帰不連続デザイン（Regression Discontinuity Design; RDD）の場合、感度分析を行う際、バンド幅を色々と調整しながら局所処置効果を推定します。RDDの詳細については[矢内の授業資料](https://yukiyanai.github.io/econometrics2/regression-discontinuity.html)、および[SONGの授業資料](https://www.jaysong.net/post/kobe2020-qpm1/)などに譲りますが、ハンド幅の調整はデータの範囲を少しずつ拡張（縮小）させながら同じ分析を繰り返すことです。以下ではアメリカ上院選挙のデータを例に、方法を紹介します。

使用するデータは{rdrobust}パッケージが提供する`rdrobust_RDsenate`というデータセットです。{pacman}パッケージの`p_load()`関数を使ってパッケージのインストールと読み込みを行い、`data()`関数を使って、データを読み込みます。

```{r}
pacman::p_load(rdrobust)  # {rdrobust}のインストール & 読み込み
data("rdrobust_RDsenate") # rdrobust_RDsenateデータの読み込み

# データをSenate_dfという名で格納
Senate_df <- as_tibble(rdrobust_RDsenate)
```

`Senate_df`の中身を確認してみます。

```{r}
Senate_df
```

本データの詳細は @Cattaneo_Frandsen_Titiunik:2015 に譲りますが、ここでは簡単に説明します。`margin`はある選挙区の民主党候補者の得票率から共和党候補者の投票率を引いたものです。100なら民主党候補者の圧勝、-100なら共和党候補者の圧勝です。これが0に近いと辛勝または惜敗となり、この辺の民主党候補者と共和党候補者は候補者としての資質や資源が近いと考えられます。`vote`は次回の選挙における民主党候補者の得票率です。ある候補者が現職であることによるアドバンテージを調べる際、「民主党が勝った選挙区における次回選挙での民主党候補者の得票率」から「民主党が負けた選挙区における次回選挙での民主党候補者の得票率」を引くだけでは不十分でしょう。圧勝できた選挙区は次回でも得票率が高いと考えられますが、これは現職というポジションによるアドバンテージ以外にも、候補者がもともと備えている高い能力・資源にも起因するからです。だから辛勝・惜敗の選挙区のみに限定することで、現職と新人の能力・資源などを出来る限り均質にし、「現職」というポジションだけが異なる状況を見つけることになります。

問題は辛勝と惜敗の基準をどう決めるかですが、広く使われている方法としては @Imbens_Kalyanaraman:2012 の最適バンド幅（optimal bandwidth）が広く使われます。しかし、最適バンド幅を使うとしても感度分析（sensitivity analysis）を行うケースも多く、この場合、バンド幅を少しずつ変えながら現職の効果を推定することになります。推定式は以下のようになります。$I(\text{margin} > 0)$は`margin`が0より大きい場合、1となる指示関数（indicator function）であす。

$$
\hat{\text{vote}} = \beta_0 + \beta_1 \cdot \text{margin} + \tau \cdot I(\text{margin} > 0) \quad \text{where} \quad -h \leq \text{margin} \leq -h
$$

それではまずは$h$が100、つまり全データを使った分析を試しにやってみましょう。

```{r}
RDD_Fit <- lm(vote ~ margin + I(margin > 0), data = Senate_df)

summary(RDD_Fit)
```

`I(margin > 0)TRUE`の係数`r round(coef(RDD_Fit)[3], 3)`が現職効果です。この作業を$h = 100$から$h = 10$まで、5ずつ変えながら分析を繰り返してみます。まずは、`RDD_df`というtibbleを作成し、`BW`列には`c(100, 95, 90, ... 10)`を入れます。続いて、`filter()`関数を利用して`Senate_df`から`margin`が`-BW`以上、`BW`以下のデータを抽出し、`Subset`という名の列として格納します。

```{r}
RDD_df <- tibble(BW = seq(100, 10, by = -5))

RDD_df <- RDD_df %>%
  mutate(Subset = map(BW, ~filter(Senate_df, 
                                  margin >= -.x & margin <= .x)))

RDD_df
```

```{r}
RDD_df <- RDD_df %>%
  mutate(
    # 各Subsetに対し、回帰分析を実施し、Model列に格納
    Model  = map(Subset, ~lm(vote ~ margin * I(margin > 0), data = .x)),
    # Model列の各セルから{broom}のtidy()で推定値のみ抽出
    Est    = map(Model, broom::tidy, conf.int = TRUE)
    ) %>%
  # Est列の入れ子構造を解除
  unnest(Est) %>% 
  # 現職効果に該当する行のみを抽出
  filter(term == "I(margin > 0)TRUE") %>%
  # 不要な列を削除
  select(!c(Subset, Model, term, statistic, p.value))

RDD_df
```

19回の回帰分析が数行のコードで簡単に実施でき、必要な情報も素早く抽出することができました。

以上の例は、交互作用なしの線形回帰分析による局所処置効果の推定例です。しかし、ノンパラメトリックRDDでは、割当変数（running variable）処置変数の交差項や割当変数の二乗項を入れる場合が多いです。今回の割当変数は`margin`です。また、閾値（cutpoint）に近いケースには高い重みを付けることが一般的な作法であり、多く使われるのが三角（triangular）カーネルです。上記の例はすべてのケースに同じ重みを付ける矩形（rectagular）カーネルを使った例です。

以上の理由により、やはりRDDは専用のパッケージを使った方が良いかも知れません。既に読み込んである{rdrobust}も推定可能ですが、ここでは{rdd}パッケージを使ってみましょう。{rdd}パッケージによるRDDは`RDestimate()`関数を使います。実際の例を確認してみましょう。`map()`を使う前に、オブジェクトの構造を把握しておくことは重要です。

```{r}
#| eval: false
# cutpoint引数の既定値は0であるため、今回の例では省略可能
RDestimate(応答変数 ~ 割当変数, data = データオブジェクト, cutpoint = 閾値, bw = バンド幅)
```

```{r}
pacman::p_load(rdd) # パッケージの読み込み

# バンド幅を指定したRDD推定
RDD_Fit <- RDestimate(vote ~ margin, data = Senate_df, bw = 100)

# RDDの推定結果を見る
summary(RDD_Fit)
```

現職効果は`r round(RDD_Fit$est[1], 3)`、その標準誤差は`r round(RDD_Fit$se[1], 3)`です。これらの情報はどこから抽出できるか確認してみます。

```{r}
# 推定値の情報がどこに格納されているかを確認
str(RDD_Fit)
```

`$est`と`$se`に私たちが探している数値が入っていますね。それぞれ抽出してみると長さ3のnumericベクトルが出力され、その中で1番目の要素がLATEということが分かります。

```{r}
# est要素の1番目の要素がLATE
RDD_Fit$est

# se要素の1番目の要素がLATEの標準誤差
RDD_Fit$se
```

それでは分析に入ります。今回も$h$を予め`BW`という名の列で格納した`RDD_df`を作成します。

```{r}
RDD_df <- tibble(BW = seq(100, 10, by = -5))
```

今回はラムダ関数を使わず、事前に関数を定義しておきましょう。関数の自作については第[-@sec-functions]章を参照してください。

```{r}
# 引数をxとするRDD_Func()の定義
RDD_Func <- function(x) {
  # バンド幅をxにしたRDD推定
  Temp_Est <- RDestimate(vote ~ margin, data = Senate_df, bw = x)
  # LATEとその標準誤差をtibbleとして返す
  tibble(LATE = Temp_Est$est[1],
         SE   = Temp_Est$se[1])
}
```

問題なく動くかを確認してみましょう。

```{r}
RDD_Func(100)
```

それでは分析をやってみましょう。今回の場合、`map()`の第二引数はラムダ関数ではないため`~`で始める必要ありません。関数名だけで十分です。

```{r}
RDD_df <- RDD_df %>%
  mutate(RDD  = map(BW, RDD_Func)) %>%
  unnest(RDD)

RDD_df
```

せっかくなので可視化してみましょう。今回は`geom_pointrange()`を使わず、`geom_line()`と`geom_ribbon()`を組み合わせます。`geom_line()`はLATEを、`geom_ribbion()`は95%信頼区間を表します。作図の前に95%信頼区間を計算しておきます。

```{r}
#| label: fig-iteration3
#| fig-cap: "LATEの推定値 (バンド幅別)"
RDD_df %>%
  mutate(CI_lwr = LATE + qnorm(0.025) * SE,
         CI_upr = LATE + qnorm(0.975) * SE) %>%
  ggplot() +
  geom_hline(yintercept = 0, color = "red") +
  geom_ribbon(aes(x = BW, ymin = CI_lwr, ymax = CI_upr), alpha = 0.5) +
  geom_line(aes(x = BW, y = LATE), size = 1) +
  labs(x = "Bandwidth", y = "Local Average Treatment Effect (%p)") +
  theme_minimal()
```

### 説明・応答変数を指定したモデル推定 {#sec-iteration-variables}

続いて、同じデータに対して推定式のみを変えた反復推定をやってみましょう。たとえば、応答変数は固定し、グループ変数のみを変えながらt検定を繰り返すケースを考えてみましょう。たとえば、OECDに加盟しているかどうか（`OECD`）で購買力平価GDP（`PPP`）の差があるかを検定してみましょう。使用する関数は`t.test()`です。第一引数には`応答変数 ~ 説明変数`とし、`data`引数にはデータフレームやtibbleオブジェクトを指定します。

```{r}
Diff_test <- t.test(PPP ~ G20, data = Country_df)

Diff_test
```

ここからいくつかの情報が読み取れます。まず、OECD加盟国のPPPは4957943、非加盟国は211287です。その差分は-4746656です。また、t値は-3.4136、p値は0.003です。これらの情報を効率よく抽出するには`broom::tidy()`が便利です。

```{r}
broom::tidy(Diff_test)
```

差分、t値、p値、95%信頼区間などが抽出できます。これをOECDだけでなく、G7やG20に対しても同じ検定を繰り返してみましょう。

`t.test()`の第一引数だけを変えながら推定をすれば良いでしょう。この第一引数、たとえば`PPP ~ G20`の方はformula型と呼ばれるRにおけるデータ型の一つです。これはcharacter型でないことに注意してください。

```{r}
#| error: true
Formula1 <- "PPP ~ G20"
t.test(Formula1, data = Country_df)
```

このようにエラーが出ます。この`Formula`を`as.formula()`関数を使ってformula型に変換し、推定をやってみると問題なく推定できることが分かります。

```{r}
#| error: true
Formula2 <- as.formula(Formula1)
t.test(Formula2, data = Country_df)
```

これから私たちがやるのは`"PPP ~ "`の次に`"G7"`、`"G20"`、`"OECD"`を`paste()`や`paste0()`関数を結合し、formula型に変換することです。formula型の列さえ生成できれば、あとは`map()`関数にformulaを渡し、データは`Country_df`に固定するだけです。

まずはどの変数を説明変数にするかを`Group`列として格納した`Diff_df`を作成します。

```{r}
Diff_df <- tibble(Group = c("G7", "G20", "OECD"))

Diff_df
```

続いて、`Formula`列を作成し、`"PPP ~ "`の次に`Group`列の文字列を結合します。

```{r}
Diff_df <- Diff_df %>%
  mutate(Formula = paste0("PPP ~ ", Group))

Diff_df
```

今の`Formula`列のデータ型を確認してみましょう。

```{r}
class(Diff_df$Formula[[1]])
```

character型ですね。続いて、`map()`関数を使用してFormula列をformula型に変換します。

```{r}
Diff_df <- Diff_df %>%
  mutate(Formula = map(Formula, as.formula))

Diff_df
```

データ型も確認してみましょう。

```{r}
class(Diff_df$Formula[[1]])
```

formula型になっていることが分かります。それではt検定を行います。ここでもラムダ関数を使います。式が入る場所には`.x`を指定し、データは`Country_df`に固定します。

```{r}
Diff_df <- Diff_df %>%
  mutate(Model = map(Formula, ~t.test(.x, data = Country_df)))

Diff_df
```


`broom::tidy()`で推定値の要約を抽出し、入れ子構造を解除します。

```{r}
Diff_df <- Diff_df %>%
  mutate(Tidy = map(Model, broom::tidy)) %>%
  unnest(Tidy)

Diff_df
```

いくつか使用しない情報もあるので、適宜列を削除します。

```{r}
Diff_df <- Diff_df %>%
  select(-c(Formula, Model, estimate1, estimate2, 
            p.value, method, alternative))

Diff_df
```

以上のコードをパイプ演算子を使用し、簡潔にまとめると以下のようになります。

```{r}
#| eval: false
Diff_df <- tibble(Group = c("G7", "G20", "OECD"))

Diff_df <- Diff_df %>%
  mutate(Formula = paste0("PPP ~ ", Group),
         Formula = map(Formula, as.formula),
         Model   = map(Formula, ~t.test(.x, data = Country_df)),
         Tidy    = map(Model, broom::tidy)) %>%
  unnest(Tidy) %>%
  select(-c(Formula, Model, estimate1, estimate2, 
            p.value, method, alternative))
```

推定結果を可視化してみましょう。

```{r}
#| label: fig-iteration4
#| fig-cap: "非加盟国と加盟国の一人当たりPPP-GDPの差分 (OECD/G20/G7)"
Diff_df %>%
  mutate(Group = fct_inorder(Group),
         Sig   = if_else(conf.low * conf.high > 0, "統計的有意",
                         "統計的非有意")) %>%
  ggplot() +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_pointrange(aes(x = estimate, xmin = conf.low, xmax = conf.high,
                      y = Group, color = Sig), size = 0.75) +
  scale_color_manual(values = c("統計的有意"   = "black",
                                "統計的非有意" = "gray70")) +
  labs(x = "非加盟国のPPP - 加盟国のPPP", y = "グループ", color = "") +
  theme_bw(base_size   = 12) +
  theme(legend.position = "bottom")
```

以上の方法を応用すると応答変数を変えながら分析を繰り返すこともできます。他にも説明変数が2つ以上のケースにも応用可能です。
